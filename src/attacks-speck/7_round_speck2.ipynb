{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7-round Speck (FNN)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 15:45:43.085476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_rr.make_train_data import make_train_data\n",
    "from dataset_rr.speck import Speck"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 10**6\n",
    "n_eval_samples = 10**5\n",
    "n_rounds = 7\n",
    "\n",
    "cipher = Speck(n_rounds=n_rounds)\n",
    "\n",
    "key = cipher.draw_keys(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, train_labels = make_train_data(n_train_samples, cipher, key)\n",
    "test_samples, test_labels = make_train_data(n_eval_samples, cipher, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Labels Shape: (1000000, 32)\n",
      "===== Label Shape: (32,)\n",
      "===== Training Samples Shape: (1000000, 32)\n",
      "===== Sample Shape: (32,)\n",
      "===== Testing Labels Shape: (100000, 32)\n",
      "===== Testing Samples Shape: (100000, 32)\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(train_labels, train_samples, test_labels, test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters\n",
    "In this code block, we specify most parameters and hyperparameters that will be used in the training of the neural network.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.shape(train_samples[0])\n",
    "\n",
    "# output dimension\n",
    "dim = len(train_labels[0])\n",
    "\n",
    "# units per hidden layer\n",
    "units = dim*16\n",
    "\n",
    "loss_scc = 'sparse_categorical_crossentropy'\n",
    "loss_mse = 'mse'\n",
    "loss_bce = 'binary_crossentropy'\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.01)\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "metrics = ['accuracy', 'binary_accuracy']\n",
    "epochs = 100\n",
    "batch_size = 5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In this code block, we create the model, according to the parameters and the topology we want to achieve. \n",
    "We then compile it specifying the optimizer, the loss and the metrics we want outputted.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 512)               16896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 32)                16416     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,083,936\n",
      "Trainable params: 1,083,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Â Type of model\n",
    "neural_network = Sequential()\n",
    "\n",
    "# Input layer\n",
    "neural_network.add(Input(shape=input_shape))\n",
    "\n",
    "# Hidden layers\n",
    "#neural_network.add(BatchNormalization())\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "neural_network.add(Dense(units=dim, activation='sigmoid'))\n",
    "\n",
    "# Summary\n",
    "neural_network.summary()\n",
    "\n",
    "# Compile model\n",
    "neural_network.compile(optimizer=optimizer, loss=loss_mse, metrics=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this code block, we train the model. It outputs, for each epoch, the loss and metrics.\n",
    "\n",
    "This block mostly stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "180/180 [==============================] - 28s 156ms/step - loss: 0.2500 - accuracy: 0.0328 - binary_accuracy: 0.5003 - val_loss: 0.2500 - val_accuracy: 3.1000e-04 - val_binary_accuracy: 0.4994\n",
      "Epoch 2/100\n",
      "180/180 [==============================] - 37s 207ms/step - loss: 0.2500 - accuracy: 0.0207 - binary_accuracy: 0.5004 - val_loss: 0.2500 - val_accuracy: 0.0016 - val_binary_accuracy: 0.4996\n",
      "Epoch 3/100\n",
      "180/180 [==============================] - 32s 178ms/step - loss: 0.2500 - accuracy: 0.0301 - binary_accuracy: 0.5007 - val_loss: 0.2500 - val_accuracy: 0.0846 - val_binary_accuracy: 0.4999\n",
      "Epoch 4/100\n",
      "180/180 [==============================] - 34s 191ms/step - loss: 0.2500 - accuracy: 0.0299 - binary_accuracy: 0.5010 - val_loss: 0.2500 - val_accuracy: 0.0013 - val_binary_accuracy: 0.4999\n",
      "Epoch 5/100\n",
      "180/180 [==============================] - 31s 173ms/step - loss: 0.2500 - accuracy: 0.0354 - binary_accuracy: 0.5014 - val_loss: 0.2500 - val_accuracy: 0.0510 - val_binary_accuracy: 0.4999\n",
      "Epoch 6/100\n",
      "180/180 [==============================] - 28s 156ms/step - loss: 0.2500 - accuracy: 0.0345 - binary_accuracy: 0.5021 - val_loss: 0.2500 - val_accuracy: 0.0169 - val_binary_accuracy: 0.4998\n",
      "Epoch 7/100\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 0.2500 - accuracy: 0.0320 - binary_accuracy: 0.5029 - val_loss: 0.2500 - val_accuracy: 0.0085 - val_binary_accuracy: 0.4998\n",
      "Epoch 8/100\n",
      "180/180 [==============================] - 27s 150ms/step - loss: 0.2500 - accuracy: 0.0275 - binary_accuracy: 0.5039 - val_loss: 0.2500 - val_accuracy: 0.0177 - val_binary_accuracy: 0.5004\n",
      "Epoch 9/100\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 0.2500 - accuracy: 0.0280 - binary_accuracy: 0.5052 - val_loss: 0.2500 - val_accuracy: 0.0404 - val_binary_accuracy: 0.5003\n",
      "Epoch 10/100\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 0.2499 - accuracy: 0.0326 - binary_accuracy: 0.5069 - val_loss: 0.2501 - val_accuracy: 0.0417 - val_binary_accuracy: 0.5003\n",
      "Epoch 11/100\n",
      "180/180 [==============================] - 27s 148ms/step - loss: 0.2499 - accuracy: 0.0312 - binary_accuracy: 0.5091 - val_loss: 0.2501 - val_accuracy: 0.0356 - val_binary_accuracy: 0.5003\n",
      "Epoch 12/100\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 0.2498 - accuracy: 0.0331 - binary_accuracy: 0.5118 - val_loss: 0.2502 - val_accuracy: 0.0327 - val_binary_accuracy: 0.5004\n",
      "Epoch 13/100\n",
      "180/180 [==============================] - 27s 148ms/step - loss: 0.2496 - accuracy: 0.0363 - binary_accuracy: 0.5151 - val_loss: 0.2503 - val_accuracy: 0.0237 - val_binary_accuracy: 0.4998\n",
      "Epoch 14/100\n",
      "180/180 [==============================] - 27s 150ms/step - loss: 0.2494 - accuracy: 0.0388 - binary_accuracy: 0.5191 - val_loss: 0.2505 - val_accuracy: 0.0349 - val_binary_accuracy: 0.4998\n",
      "Epoch 15/100\n",
      "180/180 [==============================] - 28s 156ms/step - loss: 0.2490 - accuracy: 0.0401 - binary_accuracy: 0.5237 - val_loss: 0.2508 - val_accuracy: 0.0391 - val_binary_accuracy: 0.5003\n",
      "Epoch 16/100\n",
      "180/180 [==============================] - 28s 158ms/step - loss: 0.2486 - accuracy: 0.0411 - binary_accuracy: 0.5286 - val_loss: 0.2512 - val_accuracy: 0.0358 - val_binary_accuracy: 0.5001\n",
      "Epoch 17/100\n",
      "180/180 [==============================] - 28s 157ms/step - loss: 0.2481 - accuracy: 0.0435 - binary_accuracy: 0.5337 - val_loss: 0.2517 - val_accuracy: 0.0402 - val_binary_accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "180/180 [==============================] - 27s 151ms/step - loss: 0.2474 - accuracy: 0.0452 - binary_accuracy: 0.5390 - val_loss: 0.2522 - val_accuracy: 0.0392 - val_binary_accuracy: 0.4999\n",
      "Epoch 19/100\n",
      "180/180 [==============================] - 27s 152ms/step - loss: 0.2467 - accuracy: 0.0458 - binary_accuracy: 0.5444 - val_loss: 0.2527 - val_accuracy: 0.0352 - val_binary_accuracy: 0.4996\n",
      "Epoch 20/100\n",
      "180/180 [==============================] - 26s 146ms/step - loss: 0.2460 - accuracy: 0.0476 - binary_accuracy: 0.5493 - val_loss: 0.2534 - val_accuracy: 0.0407 - val_binary_accuracy: 0.4998\n",
      "Epoch 21/100\n",
      "180/180 [==============================] - 27s 151ms/step - loss: 0.2452 - accuracy: 0.0480 - binary_accuracy: 0.5541 - val_loss: 0.2542 - val_accuracy: 0.0340 - val_binary_accuracy: 0.4998\n",
      "Epoch 22/100\n",
      "180/180 [==============================] - 27s 148ms/step - loss: 0.2444 - accuracy: 0.0481 - binary_accuracy: 0.5587 - val_loss: 0.2547 - val_accuracy: 0.0346 - val_binary_accuracy: 0.4999\n",
      "Epoch 23/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2436 - accuracy: 0.0494 - binary_accuracy: 0.5630 - val_loss: 0.2557 - val_accuracy: 0.0318 - val_binary_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "180/180 [==============================] - 26s 146ms/step - loss: 0.2428 - accuracy: 0.0492 - binary_accuracy: 0.5668 - val_loss: 0.2563 - val_accuracy: 0.0322 - val_binary_accuracy: 0.4998\n",
      "Epoch 25/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2420 - accuracy: 0.0496 - binary_accuracy: 0.5705 - val_loss: 0.2570 - val_accuracy: 0.0358 - val_binary_accuracy: 0.4998\n",
      "Epoch 26/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2413 - accuracy: 0.0496 - binary_accuracy: 0.5739 - val_loss: 0.2576 - val_accuracy: 0.0346 - val_binary_accuracy: 0.4998\n",
      "Epoch 27/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2406 - accuracy: 0.0504 - binary_accuracy: 0.5770 - val_loss: 0.2584 - val_accuracy: 0.0376 - val_binary_accuracy: 0.4995\n",
      "Epoch 28/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2399 - accuracy: 0.0511 - binary_accuracy: 0.5797 - val_loss: 0.2590 - val_accuracy: 0.0333 - val_binary_accuracy: 0.4999\n",
      "Epoch 29/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2393 - accuracy: 0.0503 - binary_accuracy: 0.5823 - val_loss: 0.2596 - val_accuracy: 0.0328 - val_binary_accuracy: 0.4996\n",
      "Epoch 30/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2387 - accuracy: 0.0505 - binary_accuracy: 0.5847 - val_loss: 0.2602 - val_accuracy: 0.0340 - val_binary_accuracy: 0.4996\n",
      "Epoch 31/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2381 - accuracy: 0.0504 - binary_accuracy: 0.5870 - val_loss: 0.2607 - val_accuracy: 0.0360 - val_binary_accuracy: 0.4993\n",
      "Epoch 32/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2376 - accuracy: 0.0512 - binary_accuracy: 0.5891 - val_loss: 0.2614 - val_accuracy: 0.0342 - val_binary_accuracy: 0.4995\n",
      "Epoch 33/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2371 - accuracy: 0.0510 - binary_accuracy: 0.5911 - val_loss: 0.2619 - val_accuracy: 0.0361 - val_binary_accuracy: 0.4996\n",
      "Epoch 34/100\n",
      "180/180 [==============================] - 25s 142ms/step - loss: 0.2366 - accuracy: 0.0510 - binary_accuracy: 0.5929 - val_loss: 0.2622 - val_accuracy: 0.0331 - val_binary_accuracy: 0.4995\n",
      "Epoch 35/100\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 0.2362 - accuracy: 0.0506 - binary_accuracy: 0.5946 - val_loss: 0.2629 - val_accuracy: 0.0317 - val_binary_accuracy: 0.4997\n",
      "Epoch 36/100\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 0.2358 - accuracy: 0.0503 - binary_accuracy: 0.5961 - val_loss: 0.2633 - val_accuracy: 0.0328 - val_binary_accuracy: 0.4991\n",
      "Epoch 37/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2354 - accuracy: 0.0510 - binary_accuracy: 0.5976 - val_loss: 0.2639 - val_accuracy: 0.0337 - val_binary_accuracy: 0.4997\n",
      "Epoch 38/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2350 - accuracy: 0.0506 - binary_accuracy: 0.5990 - val_loss: 0.2642 - val_accuracy: 0.0357 - val_binary_accuracy: 0.4995\n",
      "Epoch 39/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2346 - accuracy: 0.0503 - binary_accuracy: 0.6003 - val_loss: 0.2647 - val_accuracy: 0.0323 - val_binary_accuracy: 0.4996\n",
      "Epoch 40/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2343 - accuracy: 0.0505 - binary_accuracy: 0.6015 - val_loss: 0.2649 - val_accuracy: 0.0340 - val_binary_accuracy: 0.4995\n",
      "Epoch 41/100\n",
      "180/180 [==============================] - 27s 147ms/step - loss: 0.2340 - accuracy: 0.0503 - binary_accuracy: 0.6027 - val_loss: 0.2653 - val_accuracy: 0.0347 - val_binary_accuracy: 0.4994\n",
      "Epoch 42/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2337 - accuracy: 0.0511 - binary_accuracy: 0.6038 - val_loss: 0.2656 - val_accuracy: 0.0317 - val_binary_accuracy: 0.4997\n",
      "Epoch 43/100\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 0.2334 - accuracy: 0.0508 - binary_accuracy: 0.6047 - val_loss: 0.2659 - val_accuracy: 0.0320 - val_binary_accuracy: 0.4996\n",
      "Epoch 44/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2331 - accuracy: 0.0509 - binary_accuracy: 0.6057 - val_loss: 0.2663 - val_accuracy: 0.0317 - val_binary_accuracy: 0.4997\n",
      "Epoch 45/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2329 - accuracy: 0.0503 - binary_accuracy: 0.6066 - val_loss: 0.2666 - val_accuracy: 0.0349 - val_binary_accuracy: 0.4996\n",
      "Epoch 46/100\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 0.2326 - accuracy: 0.0511 - binary_accuracy: 0.6075 - val_loss: 0.2671 - val_accuracy: 0.0324 - val_binary_accuracy: 0.4998\n",
      "Epoch 47/100\n",
      "180/180 [==============================] - 25s 140ms/step - loss: 0.2324 - accuracy: 0.0506 - binary_accuracy: 0.6083 - val_loss: 0.2673 - val_accuracy: 0.0337 - val_binary_accuracy: 0.4997\n",
      "Epoch 48/100\n",
      "180/180 [==============================] - 25s 140ms/step - loss: 0.2321 - accuracy: 0.0505 - binary_accuracy: 0.6090 - val_loss: 0.2674 - val_accuracy: 0.0337 - val_binary_accuracy: 0.5001\n",
      "Epoch 49/100\n",
      "180/180 [==============================] - 26s 144ms/step - loss: 0.2319 - accuracy: 0.0504 - binary_accuracy: 0.6099 - val_loss: 0.2677 - val_accuracy: 0.0340 - val_binary_accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2317 - accuracy: 0.0504 - binary_accuracy: 0.6106 - val_loss: 0.2680 - val_accuracy: 0.0324 - val_binary_accuracy: 0.4999\n",
      "Epoch 51/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2315 - accuracy: 0.0502 - binary_accuracy: 0.6112 - val_loss: 0.2682 - val_accuracy: 0.0343 - val_binary_accuracy: 0.4999\n",
      "Epoch 52/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2313 - accuracy: 0.0503 - binary_accuracy: 0.6119 - val_loss: 0.2686 - val_accuracy: 0.0327 - val_binary_accuracy: 0.4998\n",
      "Epoch 53/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2312 - accuracy: 0.0508 - binary_accuracy: 0.6124 - val_loss: 0.2689 - val_accuracy: 0.0327 - val_binary_accuracy: 0.4998\n",
      "Epoch 54/100\n",
      "180/180 [==============================] - 25s 140ms/step - loss: 0.2310 - accuracy: 0.0502 - binary_accuracy: 0.6130 - val_loss: 0.2690 - val_accuracy: 0.0378 - val_binary_accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "180/180 [==============================] - 25s 140ms/step - loss: 0.2308 - accuracy: 0.0507 - binary_accuracy: 0.6136 - val_loss: 0.2690 - val_accuracy: 0.0346 - val_binary_accuracy: 0.4999\n",
      "Epoch 56/100\n",
      "180/180 [==============================] - 25s 139ms/step - loss: 0.2306 - accuracy: 0.0505 - binary_accuracy: 0.6142 - val_loss: 0.2694 - val_accuracy: 0.0320 - val_binary_accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2305 - accuracy: 0.0504 - binary_accuracy: 0.6148 - val_loss: 0.2696 - val_accuracy: 0.0318 - val_binary_accuracy: 0.5001\n",
      "Epoch 58/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2303 - accuracy: 0.0501 - binary_accuracy: 0.6152 - val_loss: 0.2698 - val_accuracy: 0.0348 - val_binary_accuracy: 0.4998\n",
      "Epoch 59/100\n",
      "180/180 [==============================] - 25s 138ms/step - loss: 0.2302 - accuracy: 0.0502 - binary_accuracy: 0.6157 - val_loss: 0.2700 - val_accuracy: 0.0346 - val_binary_accuracy: 0.4997\n",
      "Epoch 60/100\n",
      "180/180 [==============================] - 25s 138ms/step - loss: 0.2300 - accuracy: 0.0502 - binary_accuracy: 0.6162 - val_loss: 0.2702 - val_accuracy: 0.0344 - val_binary_accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2299 - accuracy: 0.0499 - binary_accuracy: 0.6166 - val_loss: 0.2702 - val_accuracy: 0.0330 - val_binary_accuracy: 0.4999\n",
      "Epoch 62/100\n",
      "180/180 [==============================] - 27s 148ms/step - loss: 0.2298 - accuracy: 0.0499 - binary_accuracy: 0.6170 - val_loss: 0.2705 - val_accuracy: 0.0351 - val_binary_accuracy: 0.4998\n",
      "Epoch 63/100\n",
      "180/180 [==============================] - 31s 171ms/step - loss: 0.2296 - accuracy: 0.0503 - binary_accuracy: 0.6175 - val_loss: 0.2707 - val_accuracy: 0.0340 - val_binary_accuracy: 0.4997\n",
      "Epoch 64/100\n",
      "180/180 [==============================] - 26s 145ms/step - loss: 0.2295 - accuracy: 0.0502 - binary_accuracy: 0.6179 - val_loss: 0.2708 - val_accuracy: 0.0330 - val_binary_accuracy: 0.5001\n",
      "Epoch 65/100\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 0.2294 - accuracy: 0.0499 - binary_accuracy: 0.6184 - val_loss: 0.2709 - val_accuracy: 0.0325 - val_binary_accuracy: 0.4999\n",
      "Epoch 66/100\n",
      "180/180 [==============================] - 29s 163ms/step - loss: 0.2293 - accuracy: 0.0499 - binary_accuracy: 0.6187 - val_loss: 0.2711 - val_accuracy: 0.0341 - val_binary_accuracy: 0.4997\n",
      "Epoch 67/100\n",
      "180/180 [==============================] - 28s 153ms/step - loss: 0.2291 - accuracy: 0.0501 - binary_accuracy: 0.6191 - val_loss: 0.2715 - val_accuracy: 0.0333 - val_binary_accuracy: 0.4999\n",
      "Epoch 68/100\n",
      "180/180 [==============================] - 26s 143ms/step - loss: 0.2290 - accuracy: 0.0498 - binary_accuracy: 0.6195 - val_loss: 0.2715 - val_accuracy: 0.0340 - val_binary_accuracy: 0.4997\n",
      "Epoch 69/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2289 - accuracy: 0.0500 - binary_accuracy: 0.6198 - val_loss: 0.2716 - val_accuracy: 0.0315 - val_binary_accuracy: 0.4999\n",
      "Epoch 70/100\n",
      "180/180 [==============================] - 25s 141ms/step - loss: 0.2288 - accuracy: 0.0502 - binary_accuracy: 0.6201 - val_loss: 0.2718 - val_accuracy: 0.0319 - val_binary_accuracy: 0.4999\n",
      "Epoch 71/100\n",
      "180/180 [==============================] - 25s 138ms/step - loss: 0.2287 - accuracy: 0.0504 - binary_accuracy: 0.6204 - val_loss: 0.2719 - val_accuracy: 0.0321 - val_binary_accuracy: 0.4999\n",
      "Epoch 72/100\n",
      "180/180 [==============================] - 25s 137ms/step - loss: 0.2286 - accuracy: 0.0504 - binary_accuracy: 0.6208 - val_loss: 0.2721 - val_accuracy: 0.0327 - val_binary_accuracy: 0.4997\n",
      "Epoch 73/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2285 - accuracy: 0.0503 - binary_accuracy: 0.6211 - val_loss: 0.2720 - val_accuracy: 0.0295 - val_binary_accuracy: 0.4998\n",
      "Epoch 74/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2284 - accuracy: 0.0499 - binary_accuracy: 0.6213 - val_loss: 0.2723 - val_accuracy: 0.0334 - val_binary_accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "180/180 [==============================] - 23s 130ms/step - loss: 0.2284 - accuracy: 0.0501 - binary_accuracy: 0.6217 - val_loss: 0.2723 - val_accuracy: 0.0325 - val_binary_accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2283 - accuracy: 0.0502 - binary_accuracy: 0.6220 - val_loss: 0.2726 - val_accuracy: 0.0331 - val_binary_accuracy: 0.4997\n",
      "Epoch 77/100\n",
      "180/180 [==============================] - 23s 127ms/step - loss: 0.2282 - accuracy: 0.0503 - binary_accuracy: 0.6223 - val_loss: 0.2727 - val_accuracy: 0.0319 - val_binary_accuracy: 0.4998\n",
      "Epoch 78/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2281 - accuracy: 0.0502 - binary_accuracy: 0.6225 - val_loss: 0.2727 - val_accuracy: 0.0349 - val_binary_accuracy: 0.4998\n",
      "Epoch 79/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2280 - accuracy: 0.0504 - binary_accuracy: 0.6229 - val_loss: 0.2728 - val_accuracy: 0.0317 - val_binary_accuracy: 0.4997\n",
      "Epoch 80/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2279 - accuracy: 0.0503 - binary_accuracy: 0.6231 - val_loss: 0.2728 - val_accuracy: 0.0347 - val_binary_accuracy: 0.4997\n",
      "Epoch 81/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2278 - accuracy: 0.0503 - binary_accuracy: 0.6234 - val_loss: 0.2731 - val_accuracy: 0.0333 - val_binary_accuracy: 0.4999\n",
      "Epoch 82/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2277 - accuracy: 0.0505 - binary_accuracy: 0.6237 - val_loss: 0.2733 - val_accuracy: 0.0322 - val_binary_accuracy: 0.4998\n",
      "Epoch 83/100\n",
      "180/180 [==============================] - 23s 130ms/step - loss: 0.2277 - accuracy: 0.0504 - binary_accuracy: 0.6238 - val_loss: 0.2732 - val_accuracy: 0.0323 - val_binary_accuracy: 0.4998\n",
      "Epoch 84/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2276 - accuracy: 0.0499 - binary_accuracy: 0.6241 - val_loss: 0.2734 - val_accuracy: 0.0344 - val_binary_accuracy: 0.4997\n",
      "Epoch 85/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2275 - accuracy: 0.0500 - binary_accuracy: 0.6244 - val_loss: 0.2734 - val_accuracy: 0.0332 - val_binary_accuracy: 0.4999\n",
      "Epoch 86/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2275 - accuracy: 0.0501 - binary_accuracy: 0.6246 - val_loss: 0.2735 - val_accuracy: 0.0318 - val_binary_accuracy: 0.4997\n",
      "Epoch 87/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2274 - accuracy: 0.0502 - binary_accuracy: 0.6248 - val_loss: 0.2735 - val_accuracy: 0.0345 - val_binary_accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2273 - accuracy: 0.0504 - binary_accuracy: 0.6251 - val_loss: 0.2737 - val_accuracy: 0.0362 - val_binary_accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2272 - accuracy: 0.0500 - binary_accuracy: 0.6253 - val_loss: 0.2737 - val_accuracy: 0.0342 - val_binary_accuracy: 0.4998\n",
      "Epoch 90/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2272 - accuracy: 0.0502 - binary_accuracy: 0.6255 - val_loss: 0.2738 - val_accuracy: 0.0329 - val_binary_accuracy: 0.4996\n",
      "Epoch 91/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2271 - accuracy: 0.0504 - binary_accuracy: 0.6258 - val_loss: 0.2739 - val_accuracy: 0.0341 - val_binary_accuracy: 0.5001\n",
      "Epoch 92/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2270 - accuracy: 0.0503 - binary_accuracy: 0.6259 - val_loss: 0.2740 - val_accuracy: 0.0316 - val_binary_accuracy: 0.4998\n",
      "Epoch 93/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2270 - accuracy: 0.0499 - binary_accuracy: 0.6261 - val_loss: 0.2741 - val_accuracy: 0.0322 - val_binary_accuracy: 0.4996\n",
      "Epoch 94/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2269 - accuracy: 0.0500 - binary_accuracy: 0.6264 - val_loss: 0.2740 - val_accuracy: 0.0336 - val_binary_accuracy: 0.4996\n",
      "Epoch 95/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2269 - accuracy: 0.0503 - binary_accuracy: 0.6265 - val_loss: 0.2745 - val_accuracy: 0.0322 - val_binary_accuracy: 0.4997\n",
      "Epoch 96/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2268 - accuracy: 0.0500 - binary_accuracy: 0.6267 - val_loss: 0.2744 - val_accuracy: 0.0347 - val_binary_accuracy: 0.4994\n",
      "Epoch 97/100\n",
      "180/180 [==============================] - 23s 128ms/step - loss: 0.2267 - accuracy: 0.0503 - binary_accuracy: 0.6269 - val_loss: 0.2745 - val_accuracy: 0.0334 - val_binary_accuracy: 0.4996\n",
      "Epoch 98/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2267 - accuracy: 0.0498 - binary_accuracy: 0.6272 - val_loss: 0.2746 - val_accuracy: 0.0332 - val_binary_accuracy: 0.4995\n",
      "Epoch 99/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2266 - accuracy: 0.0501 - binary_accuracy: 0.6272 - val_loss: 0.2747 - val_accuracy: 0.0303 - val_binary_accuracy: 0.4998\n",
      "Epoch 100/100\n",
      "180/180 [==============================] - 23s 129ms/step - loss: 0.2266 - accuracy: 0.0503 - binary_accuracy: 0.6274 - val_loss: 0.2747 - val_accuracy: 0.0331 - val_binary_accuracy: 0.4997\n"
     ]
    }
   ],
   "source": [
    "history = train_model(neural_network, train_samples, train_labels, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2500031292438507, 0.2500009834766388, 0.2500002682209015, 0.24999885261058807, 0.24999623000621796, 0.2499919980764389, 0.24998492002487183, 0.24997209012508392, 0.24995243549346924, 0.24991726875305176, 0.24985623359680176, 0.24976029992103577, 0.24960914254188538, 0.24937887489795685, 0.2490483671426773, 0.24862118065357208, 0.24807874858379364, 0.24744610488414764, 0.24673549830913544, 0.2459687441587448, 0.24518337845802307, 0.24437923729419708, 0.24356478452682495, 0.24277286231517792, 0.2420131415128708, 0.24127574265003204, 0.24058039486408234, 0.2399139553308487, 0.23929142951965332, 0.23869112133979797, 0.23812559247016907, 0.2375909835100174, 0.23709145188331604, 0.23661784827709198, 0.23617562651634216, 0.2357586771249771, 0.23536498844623566, 0.23498433828353882, 0.23463161289691925, 0.2342890053987503, 0.23395724594593048, 0.23366059362888336, 0.2333681434392929, 0.23310396075248718, 0.2328583151102066, 0.2326032966375351, 0.23236645758152008, 0.23214615881443024, 0.2319258153438568, 0.2317131906747818, 0.2315160483121872, 0.2313227504491806, 0.23115628957748413, 0.23096968233585358, 0.23080047965049744, 0.2306351512670517, 0.23048271238803864, 0.2303282618522644, 0.23018108308315277, 0.23003537952899933, 0.2298898547887802, 0.22976718842983246, 0.22963228821754456, 0.22951434552669525, 0.22938190400600433, 0.22926832735538483, 0.22914628684520721, 0.2290366291999817, 0.22893352806568146, 0.22883830964565277, 0.2287382185459137, 0.22863052785396576, 0.22854477167129517, 0.22844435274600983, 0.22835133969783783, 0.22825407981872559, 0.22816632688045502, 0.2280760556459427, 0.22799751162528992, 0.22790274024009705, 0.2278219610452652, 0.22774457931518555, 0.22767312824726105, 0.22760552167892456, 0.2275184690952301, 0.22745998203754425, 0.22737646102905273, 0.22730937600135803, 0.22723953425884247, 0.2271757870912552, 0.22709427773952484, 0.22704096138477325, 0.22698183357715607, 0.22690534591674805, 0.2268533706665039, 0.22679756581783295, 0.2267470359802246, 0.22667019069194794, 0.22663544118404388, 0.226589173078537]\n",
      "[0.25000301003456116, 0.2500031888484955, 0.2500064969062805, 0.2500050663948059, 0.2500075399875641, 0.25000762939453125, 0.25001099705696106, 0.2500159442424774, 0.25004541873931885, 0.25007712841033936, 0.25011149048805237, 0.2501758337020874, 0.2503246068954468, 0.25048014521598816, 0.25080084800720215, 0.25118979811668396, 0.25166651606559753, 0.25221166014671326, 0.25274813175201416, 0.25343427062034607, 0.2542431354522705, 0.2546776831150055, 0.2556746006011963, 0.25632038712501526, 0.2569752037525177, 0.25761762261390686, 0.2583675682544708, 0.2589757740497589, 0.2595547139644623, 0.2602331042289734, 0.2607099115848541, 0.26138216257095337, 0.2618512511253357, 0.2622433602809906, 0.26292040944099426, 0.26329734921455383, 0.2638789713382721, 0.264212429523468, 0.26468029618263245, 0.2649202048778534, 0.2653083801269531, 0.2656175494194031, 0.26591816544532776, 0.2663498818874359, 0.266577810049057, 0.26714858412742615, 0.2673337757587433, 0.267435222864151, 0.26768285036087036, 0.2679882347583771, 0.2682279944419861, 0.2686085104942322, 0.26894092559814453, 0.26897379755973816, 0.26901698112487793, 0.26939067244529724, 0.2695651948451996, 0.26976102590560913, 0.27003663778305054, 0.27019762992858887, 0.2702491283416748, 0.2705095410346985, 0.2706538140773773, 0.2707776129245758, 0.2708861231803894, 0.2710508108139038, 0.27145662903785706, 0.27152928709983826, 0.27158063650131226, 0.2717539370059967, 0.27191027998924255, 0.2720833420753479, 0.2719784080982208, 0.2723204493522644, 0.27226150035858154, 0.27255356311798096, 0.272662878036499, 0.27265113592147827, 0.27284032106399536, 0.2728028893470764, 0.27311763167381287, 0.2732778489589691, 0.2731650471687317, 0.27337315678596497, 0.27343878149986267, 0.27353495359420776, 0.2734828293323517, 0.27365055680274963, 0.2737164795398712, 0.27380838990211487, 0.2738548815250397, 0.2739596962928772, 0.27411314845085144, 0.27404868602752686, 0.2744668424129486, 0.2744230329990387, 0.27448925375938416, 0.27464598417282104, 0.27468276023864746, 0.2746676504611969]\n",
      "[0.0003100000030826777, 0.0016400000313296914, 0.08461999893188477, 0.0012799999676644802, 0.0509600006043911, 0.016860000789165497, 0.008489999920129776, 0.01769999973475933, 0.04041999951004982, 0.04171999916434288, 0.0355600006878376, 0.03265000134706497, 0.023660000413656235, 0.03488000109791756, 0.03911999985575676, 0.0357699990272522, 0.04019000008702278, 0.03922000154852867, 0.03516000136733055, 0.040699999779462814, 0.03401000052690506, 0.03460000082850456, 0.03183000162243843, 0.03217000141739845, 0.03581999987363815, 0.03463999927043915, 0.03764000162482262, 0.0333000011742115, 0.032760001718997955, 0.033969998359680176, 0.03596000000834465, 0.03424999862909317, 0.03612999990582466, 0.03305000066757202, 0.031690001487731934, 0.032829999923706055, 0.03367000073194504, 0.03571999818086624, 0.03231000155210495, 0.03401999920606613, 0.03474999964237213, 0.03172000125050545, 0.031950000673532486, 0.03173999860882759, 0.034949999302625656, 0.03243999928236008, 0.03365999832749367, 0.03373999893665314, 0.03395000100135803, 0.03237000107765198, 0.03432999923825264, 0.03265000134706497, 0.032680001109838486, 0.03776000067591667, 0.03457000106573105, 0.032030001282691956, 0.03180000185966492, 0.03480999916791916, 0.03463999927043915, 0.034370001405477524, 0.03296000137925148, 0.03505000099539757, 0.03395000100135803, 0.03300999850034714, 0.032499998807907104, 0.03407999873161316, 0.033330000936985016, 0.03395000100135803, 0.031539998948574066, 0.03189999982714653, 0.03206999972462654, 0.03274000063538551, 0.029479999095201492, 0.033399999141693115, 0.03246000036597252, 0.03311000019311905, 0.03187999874353409, 0.03491000086069107, 0.03172999992966652, 0.034699998795986176, 0.033309999853372574, 0.03220000118017197, 0.032349999994039536, 0.034380000084638596, 0.03320999816060066, 0.03175999969244003, 0.03454999998211861, 0.0361500009894371, 0.03423000127077103, 0.03294000029563904, 0.034129999577999115, 0.03158000111579895, 0.032180000096559525, 0.03359999880194664, 0.03223000094294548, 0.034710001200437546, 0.0333699993789196, 0.03322000056505203, 0.030330000445246696, 0.03305999934673309]\n"
     ]
    }
   ],
   "source": [
    "h_loss = history.history['loss']\n",
    "h_val_loss = history.history['val_loss']\n",
    "h_val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "print(h_loss)\n",
    "print(h_val_loss)\n",
    "print(h_val_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Here, we evaluate the neural network with the test data.\n",
    "\n",
    "This block stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2747 - accuracy: 0.0327 - binary_accuracy: 0.4998\n",
      "Test loss: 0.274662047624588\n",
      "Test accuracy: 0.03271999955177307\n"
     ]
    }
   ],
   "source": [
    "results = neural_network.evaluate(test_samples, test_labels, batch_size=batch_size)\n",
    "print(\"Test loss: {}\".format(results[0]))\n",
    "print(\"Test accuracy: {}\".format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = neural_network.predict(test_samples[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4525163 , 0.17726934, 0.69340664, 0.4365242 , 0.76196057,\n",
       "       0.4180476 , 0.53752935, 0.59868187, 0.35439357, 0.6448451 ,\n",
       "       0.5328131 , 0.24068972, 0.14234057, 0.79492795, 0.4710139 ,\n",
       "       0.44469225, 0.28196803, 0.42624196, 0.5659851 , 0.61326224,\n",
       "       0.8048344 , 0.08102047, 0.36428064, 0.7318592 , 0.6697646 ,\n",
       "       0.57372767, 0.24984804, 0.2781828 , 0.39109993, 0.4496559 ,\n",
       "       0.7173713 , 0.24956965], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37754977, 0.5799621 , 0.4138214 , 0.42336604, 0.20023662,\n",
       "       0.6535071 , 0.48306206, 0.38928786, 0.48633155, 0.5324746 ,\n",
       "       0.6377058 , 0.6476289 , 0.5499287 , 0.5077108 , 0.48496833,\n",
       "       0.57353663, 0.37935862, 0.50139517, 0.35069612, 0.3383927 ,\n",
       "       0.54697484, 0.61946917, 0.35271114, 0.6439532 , 0.5327635 ,\n",
       "       0.7996704 , 0.5937966 , 0.6946059 , 0.46007985, 0.43709946,\n",
       "       0.15768008, 0.64882374], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
