{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified AES Plaintext Recovery (FNN)\n",
    "In this experiment, the network tries to guess the plaintext from the ciphertext, helped with ascii per-byte correction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 16:42:49.400173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from dataset.datasets import SimplifiedAESDatasetCiphertextPlaintext\n",
    "from pipeline import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimplifiedAESDatasetCiphertextPlaintext('small')\n",
    "\n",
    "train_labels, train_samples, test_labels, test_samples = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Labels Shape: (1516543, 16)\n",
      "===== Label Shape: (16,)\n",
      "===== Training Samples Shape: (1516543, 16)\n",
      "===== Sample Shape: (16,)\n",
      "===== Testing Labels Shape: (649947, 16)\n",
      "===== Testing Samples Shape: (649947, 16)\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(train_labels, train_samples, test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels[:500]\n",
    "train_samples = train_samples[:500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters\n",
    "In this code block, we specify most parameters and hyperparameters that will be used in the training of the neural network.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimension: 16\n",
      "Units per hidden layer: 256\n"
     ]
    }
   ],
   "source": [
    "input_shape = np.shape(train_samples[0])\n",
    "\n",
    "# output dimension\n",
    "dim = len(train_labels[0])\n",
    "print(\"Output dimension: {}\".format(dim))\n",
    "\n",
    "# units per hidden layer\n",
    "units = dim*16\n",
    "print(\"Units per hidden layer: {}\".format(units))\n",
    "\n",
    "loss_scc = 'sparse_categorical_crossentropy'\n",
    "loss_mse = 'mse'\n",
    "loss_bce = 'binary_crossentropy'\n",
    "# 0.1 to 0.001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.01)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "metrics = ['accuracy', 'binary_accuracy']\n",
    "epochs = 1000\n",
    "batch_size = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In this code block, we create the model, according to the parameters and the topology we want to achieve. \n",
    "We then compile it specifying the optimizer, the loss and the metrics we want outputted.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 256)               4352      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 16)                4112      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205,840\n",
      "Trainable params: 205,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Â Type of model\n",
    "neural_network = Sequential()\n",
    "\n",
    "# Input layer\n",
    "neural_network.add(Input(shape=input_shape))\n",
    "\n",
    "# Hidden layers\n",
    "#neural_network.add(BatchNormalization())\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "neural_network.add(Dense(units=dim, activation='sigmoid'))\n",
    "\n",
    "# Summary\n",
    "neural_network.summary()\n",
    "\n",
    "# Compile model\n",
    "neural_network.compile(optimizer=optimizer, loss=loss_mse, metrics=metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this code block, we train the model. It outputs, for each epoch, the loss and metrics.\n",
    "\n",
    "This block mostly stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 1s 46ms/step - loss: 0.2320 - accuracy: 0.1444 - binary_accuracy: 0.6683 - val_loss: 0.1964 - val_accuracy: 0.2600 - val_binary_accuracy: 0.7362\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1808 - accuracy: 0.2133 - binary_accuracy: 0.7403 - val_loss: 0.1699 - val_accuracy: 0.2600 - val_binary_accuracy: 0.7538\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1668 - accuracy: 0.2133 - binary_accuracy: 0.7550 - val_loss: 0.1568 - val_accuracy: 0.2600 - val_binary_accuracy: 0.7900\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1555 - accuracy: 0.1022 - binary_accuracy: 0.7836 - val_loss: 0.1473 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.7987\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1443 - accuracy: 0.0000e+00 - binary_accuracy: 0.7942 - val_loss: 0.1367 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8000\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1338 - accuracy: 0.0000e+00 - binary_accuracy: 0.8072 - val_loss: 0.1255 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8213\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1231 - accuracy: 0.0000e+00 - binary_accuracy: 0.8304 - val_loss: 0.1161 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8512\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1134 - accuracy: 0.0000e+00 - binary_accuracy: 0.8526 - val_loss: 0.1054 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8662\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1042 - accuracy: 0.0000e+00 - binary_accuracy: 0.8660 - val_loss: 0.0993 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8725\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0964 - accuracy: 0.0000e+00 - binary_accuracy: 0.8761 - val_loss: 0.0933 - val_accuracy: 0.0000e+00 - val_binary_accuracy: 0.8825\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0899 - accuracy: 0.0111 - binary_accuracy: 0.8856 - val_loss: 0.0894 - val_accuracy: 0.0200 - val_binary_accuracy: 0.8925\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0838 - accuracy: 0.0267 - binary_accuracy: 0.8922 - val_loss: 0.0877 - val_accuracy: 0.0400 - val_binary_accuracy: 0.8763\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0791 - accuracy: 0.0956 - binary_accuracy: 0.9029 - val_loss: 0.0832 - val_accuracy: 0.0600 - val_binary_accuracy: 0.8988\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0734 - accuracy: 0.1778 - binary_accuracy: 0.9122 - val_loss: 0.0788 - val_accuracy: 0.3000 - val_binary_accuracy: 0.8938\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0684 - accuracy: 0.2422 - binary_accuracy: 0.9185 - val_loss: 0.0760 - val_accuracy: 0.1600 - val_binary_accuracy: 0.9000\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0648 - accuracy: 0.2244 - binary_accuracy: 0.9189 - val_loss: 0.0735 - val_accuracy: 0.1600 - val_binary_accuracy: 0.9075\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0596 - accuracy: 0.2667 - binary_accuracy: 0.9294 - val_loss: 0.0685 - val_accuracy: 0.1800 - val_binary_accuracy: 0.9087\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0555 - accuracy: 0.2756 - binary_accuracy: 0.9349 - val_loss: 0.0672 - val_accuracy: 0.1800 - val_binary_accuracy: 0.9137\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0518 - accuracy: 0.2733 - binary_accuracy: 0.9417 - val_loss: 0.0632 - val_accuracy: 0.1800 - val_binary_accuracy: 0.9150\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0474 - accuracy: 0.2978 - binary_accuracy: 0.9471 - val_loss: 0.0598 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9250\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0442 - accuracy: 0.2956 - binary_accuracy: 0.9518 - val_loss: 0.0590 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9275\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0415 - accuracy: 0.2822 - binary_accuracy: 0.9519 - val_loss: 0.0551 - val_accuracy: 0.1400 - val_binary_accuracy: 0.9312\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0387 - accuracy: 0.2244 - binary_accuracy: 0.9586 - val_loss: 0.0560 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9312\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0369 - accuracy: 0.2822 - binary_accuracy: 0.9581 - val_loss: 0.0512 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9375\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0344 - accuracy: 0.2356 - binary_accuracy: 0.9638 - val_loss: 0.0484 - val_accuracy: 0.1600 - val_binary_accuracy: 0.9413\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.0317 - accuracy: 0.2733 - binary_accuracy: 0.9644 - val_loss: 0.0480 - val_accuracy: 0.1800 - val_binary_accuracy: 0.9362\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0296 - accuracy: 0.2556 - binary_accuracy: 0.9694 - val_loss: 0.0468 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9413\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0273 - accuracy: 0.2778 - binary_accuracy: 0.9728 - val_loss: 0.0447 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9450\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0249 - accuracy: 0.2556 - binary_accuracy: 0.9747 - val_loss: 0.0433 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9500\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0234 - accuracy: 0.2600 - binary_accuracy: 0.9779 - val_loss: 0.0419 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9538\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0216 - accuracy: 0.2578 - binary_accuracy: 0.9804 - val_loss: 0.0409 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9538\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0203 - accuracy: 0.2378 - binary_accuracy: 0.9806 - val_loss: 0.0407 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9525\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 0.2644 - binary_accuracy: 0.9822 - val_loss: 0.0399 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9538\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0177 - accuracy: 0.2511 - binary_accuracy: 0.9846 - val_loss: 0.0394 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9550\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0166 - accuracy: 0.2178 - binary_accuracy: 0.9857 - val_loss: 0.0378 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9563\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0156 - accuracy: 0.2089 - binary_accuracy: 0.9874 - val_loss: 0.0377 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9588\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0149 - accuracy: 0.2156 - binary_accuracy: 0.9882 - val_loss: 0.0374 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9538\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0141 - accuracy: 0.2200 - binary_accuracy: 0.9883 - val_loss: 0.0359 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9588\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0131 - accuracy: 0.2067 - binary_accuracy: 0.9900 - val_loss: 0.0360 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9563\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0123 - accuracy: 0.2156 - binary_accuracy: 0.9900 - val_loss: 0.0348 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9613\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0115 - accuracy: 0.2022 - binary_accuracy: 0.9921 - val_loss: 0.0350 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9625\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0107 - accuracy: 0.2133 - binary_accuracy: 0.9926 - val_loss: 0.0340 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9625\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0100 - accuracy: 0.2267 - binary_accuracy: 0.9928 - val_loss: 0.0334 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9638\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0096 - accuracy: 0.2178 - binary_accuracy: 0.9928 - val_loss: 0.0330 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9650\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0091 - accuracy: 0.2244 - binary_accuracy: 0.9935 - val_loss: 0.0331 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9625\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0086 - accuracy: 0.2111 - binary_accuracy: 0.9935 - val_loss: 0.0325 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9650\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0082 - accuracy: 0.2089 - binary_accuracy: 0.9937 - val_loss: 0.0323 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9638\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0080 - accuracy: 0.2089 - binary_accuracy: 0.9943 - val_loss: 0.0317 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0075 - accuracy: 0.2178 - binary_accuracy: 0.9943 - val_loss: 0.0316 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9663\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0071 - accuracy: 0.2111 - binary_accuracy: 0.9946 - val_loss: 0.0323 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - accuracy: 0.2111 - binary_accuracy: 0.9950 - val_loss: 0.0312 - val_accuracy: 0.1800 - val_binary_accuracy: 0.9663\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0067 - accuracy: 0.1911 - binary_accuracy: 0.9951 - val_loss: 0.0320 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9625\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0064 - accuracy: 0.2311 - binary_accuracy: 0.9954 - val_loss: 0.0310 - val_accuracy: 0.2200 - val_binary_accuracy: 0.9638\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0062 - accuracy: 0.2067 - binary_accuracy: 0.9954 - val_loss: 0.0317 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9613\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0061 - accuracy: 0.2311 - binary_accuracy: 0.9954 - val_loss: 0.0304 - val_accuracy: 0.2000 - val_binary_accuracy: 0.9650\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0059 - accuracy: 0.2178 - binary_accuracy: 0.9954 - val_loss: 0.0311 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0057 - accuracy: 0.2333 - binary_accuracy: 0.9954 - val_loss: 0.0305 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0056 - accuracy: 0.2289 - binary_accuracy: 0.9954 - val_loss: 0.0309 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9625\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0055 - accuracy: 0.2489 - binary_accuracy: 0.9954 - val_loss: 0.0303 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9650\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0054 - accuracy: 0.2289 - binary_accuracy: 0.9954 - val_loss: 0.0303 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9638\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0052 - accuracy: 0.2844 - binary_accuracy: 0.9956 - val_loss: 0.0298 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9663\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0051 - accuracy: 0.2644 - binary_accuracy: 0.9958 - val_loss: 0.0299 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0050 - accuracy: 0.2267 - binary_accuracy: 0.9958 - val_loss: 0.0303 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9650\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0049 - accuracy: 0.2422 - binary_accuracy: 0.9958 - val_loss: 0.0292 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9663\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0048 - accuracy: 0.2444 - binary_accuracy: 0.9958 - val_loss: 0.0298 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9663\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0047 - accuracy: 0.2289 - binary_accuracy: 0.9960 - val_loss: 0.0298 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9675\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0046 - accuracy: 0.2422 - binary_accuracy: 0.9961 - val_loss: 0.0297 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9663\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - accuracy: 0.2422 - binary_accuracy: 0.9961 - val_loss: 0.0293 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0045 - accuracy: 0.2533 - binary_accuracy: 0.9961 - val_loss: 0.0296 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9663\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0045 - accuracy: 0.2578 - binary_accuracy: 0.9961 - val_loss: 0.0295 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0044 - accuracy: 0.2600 - binary_accuracy: 0.9961 - val_loss: 0.0296 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9663\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0044 - accuracy: 0.2511 - binary_accuracy: 0.9961 - val_loss: 0.0294 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9663\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0043 - accuracy: 0.2556 - binary_accuracy: 0.9961 - val_loss: 0.0294 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0042 - accuracy: 0.2600 - binary_accuracy: 0.9962 - val_loss: 0.0292 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0042 - accuracy: 0.2644 - binary_accuracy: 0.9962 - val_loss: 0.0294 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0042 - accuracy: 0.2667 - binary_accuracy: 0.9962 - val_loss: 0.0293 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9675\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0041 - accuracy: 0.2600 - binary_accuracy: 0.9962 - val_loss: 0.0293 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0041 - accuracy: 0.2556 - binary_accuracy: 0.9962 - val_loss: 0.0294 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9675\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0040 - accuracy: 0.2489 - binary_accuracy: 0.9962 - val_loss: 0.0291 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0040 - accuracy: 0.2622 - binary_accuracy: 0.9962 - val_loss: 0.0295 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9675\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0040 - accuracy: 0.2622 - binary_accuracy: 0.9964 - val_loss: 0.0295 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0039 - accuracy: 0.2556 - binary_accuracy: 0.9964 - val_loss: 0.0300 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9663\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0039 - accuracy: 0.2733 - binary_accuracy: 0.9965 - val_loss: 0.0303 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0038 - accuracy: 0.2733 - binary_accuracy: 0.9967 - val_loss: 0.0301 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0037 - accuracy: 0.2644 - binary_accuracy: 0.9967 - val_loss: 0.0297 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0037 - accuracy: 0.2689 - binary_accuracy: 0.9967 - val_loss: 0.0298 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.2711 - binary_accuracy: 0.9967 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0036 - accuracy: 0.2711 - binary_accuracy: 0.9967 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0035 - accuracy: 0.2667 - binary_accuracy: 0.9968 - val_loss: 0.0303 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - accuracy: 0.2689 - binary_accuracy: 0.9968 - val_loss: 0.0308 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - accuracy: 0.2600 - binary_accuracy: 0.9968 - val_loss: 0.0304 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0033 - accuracy: 0.2556 - binary_accuracy: 0.9969 - val_loss: 0.0309 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0033 - accuracy: 0.2644 - binary_accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 0.2511 - binary_accuracy: 0.9969 - val_loss: 0.0299 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9663\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 0.2689 - binary_accuracy: 0.9972 - val_loss: 0.0298 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9663\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 0.2800 - binary_accuracy: 0.9972 - val_loss: 0.0300 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9663\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 0.2756 - binary_accuracy: 0.9972 - val_loss: 0.0300 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.2667 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.2667 - binary_accuracy: 0.9972 - val_loss: 0.0304 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.2622 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0030 - accuracy: 0.2689 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 0.2711 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.2689 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2689 - binary_accuracy: 0.9972 - val_loss: 0.0300 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - accuracy: 0.2667 - binary_accuracy: 0.9972 - val_loss: 0.0303 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2711 - binary_accuracy: 0.9972 - val_loss: 0.0303 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - accuracy: 0.2622 - binary_accuracy: 0.9972 - val_loss: 0.0301 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2733 - binary_accuracy: 0.9972 - val_loss: 0.0301 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2756 - binary_accuracy: 0.9972 - val_loss: 0.0300 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2667 - binary_accuracy: 0.9972 - val_loss: 0.0301 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0029 - accuracy: 0.2667 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - accuracy: 0.2800 - binary_accuracy: 0.9972 - val_loss: 0.0303 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0028 - accuracy: 0.2689 - binary_accuracy: 0.9972 - val_loss: 0.0302 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 0.2800 - binary_accuracy: 0.9974 - val_loss: 0.0304 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 0.2778 - binary_accuracy: 0.9975 - val_loss: 0.0301 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9638\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0027 - accuracy: 0.2667 - binary_accuracy: 0.9975 - val_loss: 0.0307 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0027 - accuracy: 0.2711 - binary_accuracy: 0.9975 - val_loss: 0.0305 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0027 - accuracy: 0.2844 - binary_accuracy: 0.9975 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9638\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 0.2844 - binary_accuracy: 0.9975 - val_loss: 0.0301 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9638\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 0.2756 - binary_accuracy: 0.9975 - val_loss: 0.0302 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - accuracy: 0.2822 - binary_accuracy: 0.9975 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9638\n",
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - accuracy: 0.2756 - binary_accuracy: 0.9975 - val_loss: 0.0299 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9663\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0026 - accuracy: 0.2733 - binary_accuracy: 0.9975 - val_loss: 0.0300 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 0.2778 - binary_accuracy: 0.9975 - val_loss: 0.0295 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0025 - accuracy: 0.2778 - binary_accuracy: 0.9976 - val_loss: 0.0297 - val_accuracy: 0.2400 - val_binary_accuracy: 0.9650\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - accuracy: 0.2778 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - accuracy: 0.2911 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 0.2911 - binary_accuracy: 0.9976 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9663\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 0.2867 - binary_accuracy: 0.9976 - val_loss: 0.0299 - val_accuracy: 0.2600 - val_binary_accuracy: 0.9650\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0024 - accuracy: 0.2889 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9663\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 0.2978 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0024 - accuracy: 0.3022 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0024 - accuracy: 0.3000 - binary_accuracy: 0.9976 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - accuracy: 0.3133 - binary_accuracy: 0.9976 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0024 - accuracy: 0.3022 - binary_accuracy: 0.9976 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3089 - binary_accuracy: 0.9978 - val_loss: 0.0301 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3044 - binary_accuracy: 0.9978 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.2978 - binary_accuracy: 0.9978 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 142/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0023 - accuracy: 0.3044 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0302 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3044 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.2978 - binary_accuracy: 0.9978 - val_loss: 0.0304 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0023 - accuracy: 0.3022 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3089 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3089 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3044 - binary_accuracy: 0.9978 - val_loss: 0.0304 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3000 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3089 - binary_accuracy: 0.9978 - val_loss: 0.0304 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0304 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3044 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3133 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3200 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3111 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3156 - binary_accuracy: 0.9978 - val_loss: 0.0304 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3178 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3111 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3111 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9638\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3111 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3156 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3200 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3200 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3133 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3089 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0023 - accuracy: 0.3111 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3244 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.2800 - val_binary_accuracy: 0.9650\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3200 - binary_accuracy: 0.9978 - val_loss: 0.0305 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3067 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0306 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0307 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9650\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3289 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0308 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - accuracy: 0.3333 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3422 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0309 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3422 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 235/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3378 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3356 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3422 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3422 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0310 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 0.0022 - accuracy: 0.3400 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3422 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3444 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0312 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3467 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3578 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3489 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3533 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0313 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3511 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 321/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 322/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 323/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3556 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 324/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 325/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 326/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 327/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 328/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 329/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 330/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 331/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 332/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 333/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 334/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 335/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 336/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 337/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 338/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9638\n",
      "Epoch 339/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 340/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9638\n",
      "Epoch 341/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9638\n",
      "Epoch 342/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9638\n",
      "Epoch 343/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 344/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 345/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 346/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9650\n",
      "Epoch 347/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9650\n",
      "Epoch 348/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 349/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 350/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9638\n",
      "Epoch 351/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 352/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 353/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 354/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 355/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 356/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 357/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 358/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 359/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 360/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 361/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 362/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 363/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3600 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 364/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 365/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 366/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 367/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 368/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 369/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 370/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 371/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 372/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 373/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 374/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 375/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 376/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 377/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3622 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 378/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9638\n",
      "Epoch 379/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 380/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 381/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3644 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 382/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 383/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 384/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 385/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 386/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 387/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 388/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 389/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 390/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 391/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 392/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 393/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 394/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 395/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 396/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3667 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 397/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 398/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 399/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 400/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 401/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 402/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 403/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 404/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 405/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 406/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 407/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 408/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 409/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 410/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 411/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 412/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 413/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3711 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 414/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3689 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 415/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 416/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 417/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 418/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 419/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 420/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 421/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 422/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3733 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 423/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 424/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 425/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 426/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 427/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 428/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3822 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 429/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 430/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 431/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 432/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3822 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 433/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 434/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3844 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 435/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 436/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 437/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3844 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 438/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 439/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 440/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 441/1000\n",
      "5/5 [==============================] - 0s 22ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 442/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0022 - accuracy: 0.3844 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 443/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3756 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 444/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 445/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3933 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 446/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 447/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 448/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0316 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 449/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 450/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4089 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 451/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 452/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 453/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 454/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 455/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4089 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 456/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 457/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - accuracy: 0.3933 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 458/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3800 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 459/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 460/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3911 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 461/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3844 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 462/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3978 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 463/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 464/1000\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 465/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 466/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3889 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 467/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 468/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 469/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3889 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 470/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 471/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3778 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 472/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 473/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 474/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 475/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 476/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 477/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 478/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3933 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 479/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 480/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3978 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 481/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 482/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 483/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 484/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 485/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0022 - accuracy: 0.4133 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 486/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 487/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 488/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3978 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 489/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 490/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 491/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 492/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4133 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 493/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.3911 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 494/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3889 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 495/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 496/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 497/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4156 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 498/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4156 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 499/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 500/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.3933 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 501/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4200 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 502/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 503/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0022 - accuracy: 0.3956 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 504/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 505/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 506/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4267 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 507/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 508/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 509/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3867 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 510/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4267 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 511/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 512/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 513/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 514/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 515/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4244 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 516/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4133 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 517/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4111 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 518/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 519/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4244 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 520/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4244 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 521/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 522/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 523/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 524/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 525/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4089 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 526/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3933 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 527/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4044 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 528/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4178 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 529/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4022 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 530/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 531/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4000 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 532/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 533/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4089 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 534/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 535/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9625\n",
      "Epoch 536/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4267 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 537/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4156 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 538/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4133 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 539/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0022 - accuracy: 0.4067 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 540/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - accuracy: 0.4200 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 541/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4133 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 542/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4267 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 543/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4311 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 544/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 545/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4356 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 546/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 547/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4400 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 548/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.4356 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 549/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4356 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 550/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 551/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4267 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 552/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4311 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 553/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4400 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 554/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4400 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 555/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4311 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 556/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0022 - accuracy: 0.4356 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 557/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 558/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4422 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 559/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4422 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 560/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 561/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4244 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 562/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 563/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9625\n",
      "Epoch 564/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4400 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 565/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9638\n",
      "Epoch 566/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9638\n",
      "Epoch 567/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4311 - binary_accuracy: 0.9978 - val_loss: 0.0319 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9638\n",
      "Epoch 568/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4378 - binary_accuracy: 0.9978 - val_loss: 0.0318 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9638\n",
      "Epoch 569/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4422 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9638\n",
      "Epoch 570/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - accuracy: 0.4400 - binary_accuracy: 0.9978 - val_loss: 0.0315 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9638\n",
      "Epoch 571/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.4333 - binary_accuracy: 0.9978 - val_loss: 0.0317 - val_accuracy: 0.4800 - val_binary_accuracy: 0.9638\n",
      "Epoch 572/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0048 - accuracy: 0.4244 - binary_accuracy: 0.9944 - val_loss: 0.0576 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9350\n",
      "Epoch 573/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0554 - accuracy: 0.4111 - binary_accuracy: 0.9382 - val_loss: 0.0883 - val_accuracy: 0.3000 - val_binary_accuracy: 0.8963\n",
      "Epoch 574/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0559 - accuracy: 0.2111 - binary_accuracy: 0.9361 - val_loss: 0.0687 - val_accuracy: 0.4600 - val_binary_accuracy: 0.9125\n",
      "Epoch 575/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0373 - accuracy: 0.3267 - binary_accuracy: 0.9572 - val_loss: 0.0512 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9400\n",
      "Epoch 576/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0269 - accuracy: 0.3267 - binary_accuracy: 0.9715 - val_loss: 0.0459 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9413\n",
      "Epoch 577/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0164 - accuracy: 0.3111 - binary_accuracy: 0.9822 - val_loss: 0.0379 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9550\n",
      "Epoch 578/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 0.2978 - binary_accuracy: 0.9893 - val_loss: 0.0353 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9600\n",
      "Epoch 579/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0071 - accuracy: 0.3000 - binary_accuracy: 0.9940 - val_loss: 0.0327 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9600\n",
      "Epoch 580/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0056 - accuracy: 0.3200 - binary_accuracy: 0.9958 - val_loss: 0.0321 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9613\n",
      "Epoch 581/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0045 - accuracy: 0.3044 - binary_accuracy: 0.9961 - val_loss: 0.0328 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9613\n",
      "Epoch 582/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0038 - accuracy: 0.3156 - binary_accuracy: 0.9968 - val_loss: 0.0321 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9588\n",
      "Epoch 583/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0035 - accuracy: 0.3200 - binary_accuracy: 0.9968 - val_loss: 0.0316 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9613\n",
      "Epoch 584/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - accuracy: 0.3111 - binary_accuracy: 0.9969 - val_loss: 0.0313 - val_accuracy: 0.3400 - val_binary_accuracy: 0.9625\n",
      "Epoch 585/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0032 - accuracy: 0.3067 - binary_accuracy: 0.9971 - val_loss: 0.0310 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9613\n",
      "Epoch 586/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0031 - accuracy: 0.3111 - binary_accuracy: 0.9971 - val_loss: 0.0315 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 587/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0030 - accuracy: 0.3178 - binary_accuracy: 0.9971 - val_loss: 0.0320 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9638\n",
      "Epoch 588/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 0.3178 - binary_accuracy: 0.9971 - val_loss: 0.0320 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9600\n",
      "Epoch 589/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - accuracy: 0.3133 - binary_accuracy: 0.9971 - val_loss: 0.0318 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9613\n",
      "Epoch 590/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - accuracy: 0.3089 - binary_accuracy: 0.9971 - val_loss: 0.0316 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9613\n",
      "Epoch 591/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0027 - accuracy: 0.3044 - binary_accuracy: 0.9972 - val_loss: 0.0315 - val_accuracy: 0.3000 - val_binary_accuracy: 0.9600\n",
      "Epoch 592/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0024 - accuracy: 0.3000 - binary_accuracy: 0.9976 - val_loss: 0.0314 - val_accuracy: 0.3600 - val_binary_accuracy: 0.9650\n",
      "Epoch 593/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - accuracy: 0.3022 - binary_accuracy: 0.9978 - val_loss: 0.0303 - val_accuracy: 0.3200 - val_binary_accuracy: 0.9663\n",
      "Epoch 594/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3156 - binary_accuracy: 0.9979 - val_loss: 0.0305 - val_accuracy: 0.3800 - val_binary_accuracy: 0.9625\n",
      "Epoch 595/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0022 - accuracy: 0.3311 - binary_accuracy: 0.9979 - val_loss: 0.0309 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 596/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.3356 - binary_accuracy: 0.9979 - val_loss: 0.0310 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 597/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0311 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 598/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3511 - binary_accuracy: 0.9979 - val_loss: 0.0311 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9613\n",
      "Epoch 599/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0312 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9613\n",
      "Epoch 600/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3511 - binary_accuracy: 0.9979 - val_loss: 0.0313 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9613\n",
      "Epoch 601/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0313 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 602/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0314 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 603/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3467 - binary_accuracy: 0.9979 - val_loss: 0.0315 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 604/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0315 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 605/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3578 - binary_accuracy: 0.9979 - val_loss: 0.0315 - val_accuracy: 0.4000 - val_binary_accuracy: 0.9625\n",
      "Epoch 606/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3600 - binary_accuracy: 0.9979 - val_loss: 0.0316 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 607/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0316 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 608/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0316 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 609/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0316 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 610/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0317 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 611/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3533 - binary_accuracy: 0.9979 - val_loss: 0.0317 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 612/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3556 - binary_accuracy: 0.9979 - val_loss: 0.0317 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 613/1000\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0021 - accuracy: 0.3578 - binary_accuracy: 0.9979 - val_loss: 0.0318 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 614/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3689 - binary_accuracy: 0.9979 - val_loss: 0.0318 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 615/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3956 - binary_accuracy: 0.9979 - val_loss: 0.0318 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 616/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3578 - binary_accuracy: 0.9979 - val_loss: 0.0318 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 617/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3756 - binary_accuracy: 0.9979 - val_loss: 0.0319 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 618/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3756 - binary_accuracy: 0.9979 - val_loss: 0.0319 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 619/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0021 - accuracy: 0.3667 - binary_accuracy: 0.9979 - val_loss: 0.0319 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 620/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.3844 - binary_accuracy: 0.9979 - val_loss: 0.0319 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 621/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3733 - binary_accuracy: 0.9979 - val_loss: 0.0319 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 622/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3933 - binary_accuracy: 0.9979 - val_loss: 0.0320 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 623/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3978 - binary_accuracy: 0.9979 - val_loss: 0.0320 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 624/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3844 - binary_accuracy: 0.9979 - val_loss: 0.0320 - val_accuracy: 0.4200 - val_binary_accuracy: 0.9625\n",
      "Epoch 625/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3844 - binary_accuracy: 0.9979 - val_loss: 0.0320 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 626/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3800 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 627/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.3822 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 628/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.3778 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.4400 - val_binary_accuracy: 0.9625\n",
      "Epoch 629/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4067 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 630/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 631/1000\n",
      "5/5 [==============================] - 0s 39ms/step - loss: 0.0021 - accuracy: 0.4133 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 632/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4133 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5000 - val_binary_accuracy: 0.9625\n",
      "Epoch 633/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0321 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 634/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4067 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 635/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4200 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 636/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 637/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 638/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4222 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 639/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4133 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 640/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4178 - binary_accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 641/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4156 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 642/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4000 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 643/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4089 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9625\n",
      "Epoch 644/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9625\n",
      "Epoch 645/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4244 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 646/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4289 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9613\n",
      "Epoch 647/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0021 - accuracy: 0.4200 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 648/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 0.4222 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9613\n",
      "Epoch 649/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4222 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 650/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4333 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 651/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4222 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 652/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4178 - binary_accuracy: 0.9979 - val_loss: 0.0323 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 653/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4222 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 654/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4244 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5200 - val_binary_accuracy: 0.9613\n",
      "Epoch 655/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4111 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 656/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4244 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 657/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4289 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 658/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4289 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 659/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4311 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 660/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4289 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 661/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0021 - accuracy: 0.4267 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 662/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4311 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 663/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4200 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 664/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4356 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 665/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4289 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 666/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4400 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 667/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4333 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 668/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4422 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 669/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4378 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 670/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 671/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4356 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 672/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4422 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 673/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4489 - binary_accuracy: 0.9979 - val_loss: 0.0324 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 674/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4400 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 675/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4444 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 676/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4489 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 677/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4444 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 678/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 0.4422 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 679/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0021 - accuracy: 0.4489 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5400 - val_binary_accuracy: 0.9613\n",
      "Epoch 680/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4489 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 681/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4444 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 682/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4400 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 683/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 684/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4444 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 685/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4422 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 686/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 687/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4467 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 688/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4467 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 689/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 690/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4489 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 691/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4556 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 692/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 693/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4644 - binary_accuracy: 0.9979 - val_loss: 0.0325 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 694/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4556 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 695/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 696/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4600 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 697/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4467 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 698/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4444 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 699/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 700/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4467 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 701/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 702/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4622 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 703/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 704/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 705/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 706/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4600 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 707/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0021 - accuracy: 0.4556 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 708/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 709/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 710/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4556 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 711/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 712/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 713/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4600 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 714/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 715/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 716/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 717/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 718/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4644 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 719/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4533 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 720/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4511 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 721/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4578 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 722/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4622 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 723/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4600 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 724/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0021 - accuracy: 0.4644 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 725/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4622 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 726/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4600 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 727/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4622 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 728/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 729/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9613\n",
      "Epoch 730/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 731/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 732/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 733/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 734/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 735/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 736/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 737/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 738/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0326 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 739/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 740/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 741/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 742/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 743/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 744/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 745/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 746/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 747/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 748/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 749/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 750/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 751/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 752/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 753/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 754/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 755/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 756/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 757/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 758/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 759/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 760/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 761/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 762/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 763/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 764/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 765/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 766/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 767/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 768/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 769/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 770/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 771/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 772/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 773/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 774/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 775/1000\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 776/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 777/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 778/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 779/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 780/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 781/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 782/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 783/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 784/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 785/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 786/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 787/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 788/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 789/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 790/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 791/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 792/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 793/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 794/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 795/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 796/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 797/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 798/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 799/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 800/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 801/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 802/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 803/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 804/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 805/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 806/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 807/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 808/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4667 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 809/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 810/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 811/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 812/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 813/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 814/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 815/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 816/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 817/1000\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 818/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 819/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 820/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0327 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 821/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 822/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4689 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 823/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 824/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 825/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 826/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 827/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 828/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 829/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 830/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 831/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 832/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 833/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 834/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 835/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 836/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 837/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 838/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 839/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 840/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 841/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 842/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 843/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 844/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 845/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 846/1000\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 847/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 848/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 849/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 850/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 851/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 852/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 853/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 854/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 855/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 856/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 857/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 858/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 859/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 860/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 861/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 862/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 863/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 864/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4711 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 865/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 866/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 867/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 868/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 869/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 870/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 871/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4733 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 872/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 873/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 874/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9625\n",
      "Epoch 875/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 876/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 877/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 878/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 879/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 880/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 881/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 882/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 883/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 884/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 885/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 886/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 887/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 888/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 889/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 890/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 891/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 892/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 893/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 894/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 895/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 896/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 897/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 898/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 899/1000\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 900/1000\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 901/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 902/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 903/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 904/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 905/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 906/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 907/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 908/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 909/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 910/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 911/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 912/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 913/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 914/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4756 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 915/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 916/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 917/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 918/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 919/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 920/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 921/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 922/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 923/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 924/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 925/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 926/1000\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 927/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 928/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 929/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 930/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 931/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 932/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 933/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 934/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 935/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 936/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 937/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4778 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 938/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 939/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 940/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 941/1000\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 942/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 943/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 944/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 945/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 946/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 947/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 948/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 949/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 950/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 951/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 952/1000\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 953/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 954/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 955/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 956/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 957/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 958/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 959/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 960/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 961/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 962/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 963/1000\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 964/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 965/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 966/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 967/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 968/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 969/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 970/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 971/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 972/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 973/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 974/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 975/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 976/1000\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 977/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 978/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 979/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 980/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4800 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 981/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 982/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 983/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4844 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 984/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 985/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4978 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 986/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4933 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 987/1000\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 0.0021 - accuracy: 0.4978 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 988/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4956 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 989/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 990/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4822 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 991/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 992/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4933 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 993/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 994/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 995/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 996/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - accuracy: 0.4933 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5600 - val_binary_accuracy: 0.9638\n",
      "Epoch 997/1000\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 0.0021 - accuracy: 0.4889 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 998/1000\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0021 - accuracy: 0.4978 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 999/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4911 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n",
      "Epoch 1000/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0021 - accuracy: 0.4867 - binary_accuracy: 0.9979 - val_loss: 0.0328 - val_accuracy: 0.5800 - val_binary_accuracy: 0.9638\n"
     ]
    }
   ],
   "source": [
    "history = train_model(neural_network, train_samples, train_labels, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2320386916399002, 0.18079587817192078, 0.16680674254894257, 0.15546691417694092, 0.14429835975170135, 0.13375793397426605, 0.12314662337303162, 0.11337495595216751, 0.10415776073932648, 0.09636140614748001, 0.08985955268144608, 0.08383307605981827, 0.07905638962984085, 0.07339588552713394, 0.0684218481183052, 0.06477036327123642, 0.05957719683647156, 0.055506594479084015, 0.0517854318022728, 0.047369834035634995, 0.04424929618835449, 0.04152318462729454, 0.03869403153657913, 0.03693920373916626, 0.034351855516433716, 0.031724829226732254, 0.029562581330537796, 0.027305420488119125, 0.024921929463744164, 0.023350486531853676, 0.021556388586759567, 0.020302096381783485, 0.019039414823055267, 0.017653847113251686, 0.016641756519675255, 0.015626797452569008, 0.014932402409613132, 0.014097281731665134, 0.01309002935886383, 0.012253175489604473, 0.011526775546371937, 0.01074199564754963, 0.00998299103230238, 0.009639191441237926, 0.009098876267671585, 0.008606280200183392, 0.008193661458790302, 0.007983402349054813, 0.007524840999394655, 0.007128935307264328, 0.006853253580629826, 0.006662033963948488, 0.006410067901015282, 0.006207649130374193, 0.0060628256760537624, 0.005874203983694315, 0.005726568400859833, 0.005612950772047043, 0.0055420114658772945, 0.005407999269664288, 0.005247428081929684, 0.005129276774823666, 0.005013384390622377, 0.0048811547458171844, 0.004823495168238878, 0.004657119978219271, 0.004605446010828018, 0.004533838480710983, 0.004505222197622061, 0.004472257103770971, 0.004418583586812019, 0.004366259090602398, 0.004303824622184038, 0.004235539119690657, 0.004200567491352558, 0.00418699299916625, 0.004147072788327932, 0.004105243366211653, 0.004046515561640263, 0.003995950799435377, 0.003982875496149063, 0.003922091331332922, 0.0039039410185068846, 0.0037719267420470715, 0.0037182255182415247, 0.0036616665311157703, 0.003630914492532611, 0.0036007261369377375, 0.003486065659672022, 0.003443456022068858, 0.003424179507419467, 0.003343935590237379, 0.0032954588532447815, 0.003217614023014903, 0.003134177764877677, 0.0030990249942988157, 0.0030425828881561756, 0.003015447873622179, 0.0030042161233723164, 0.002990329870954156, 0.002977971453219652, 0.0029611699283123016, 0.002953071380034089, 0.002946325344964862, 0.0029362847562879324, 0.002928489586338401, 0.0029223845340311527, 0.002914575394243002, 0.0029063173569738865, 0.002898718463256955, 0.0028840939048677683, 0.002862313762307167, 0.0028217248618602753, 0.002726899227127433, 0.0027343553956598043, 0.002713507506996393, 0.0026697961147874594, 0.002651681425049901, 0.0026375765446573496, 0.0026179547421634197, 0.002606278285384178, 0.002590623451396823, 0.002570119686424732, 0.0025299217086285353, 0.002493464620783925, 0.0024733911268413067, 0.0024704893585294485, 0.002461901167407632, 0.002453966299071908, 0.0024493937380611897, 0.0024436942767351866, 0.00243698013946414, 0.0024237241595983505, 0.0024585684295743704, 0.002383032813668251, 0.002334577264264226, 0.002316516125574708, 0.002309979172423482, 0.0023051388561725616, 0.0022984854876995087, 0.0022945443633943796, 0.0022911059204488993, 0.0022879205644130707, 0.0022854169365018606, 0.002283492125570774, 0.002281537279486656, 0.0022799335420131683, 0.002278541214764118, 0.002276828046888113, 0.002275338862091303, 0.0022745481692254543, 0.002273078076541424, 0.002271966775879264, 0.0022708866745233536, 0.002269798656925559, 0.002268912037834525, 0.0022680582478642464, 0.002266978845000267, 0.002266084076836705, 0.0022653494961559772, 0.002264351351186633, 0.0022637040819972754, 0.002262949477881193, 0.0022623096592724323, 0.002261462388560176, 0.0022607073187828064, 0.00226002954877913, 0.002259337343275547, 0.0022588283754885197, 0.0022581438533961773, 0.002257524523884058, 0.0022570528090000153, 0.002256384352222085, 0.0022558991331607103, 0.0022552725858986378, 0.0022547675762325525, 0.0022542078513652086, 0.002253809478133917, 0.0022533664014190435, 0.0022528325207531452, 0.002252446487545967, 0.002251931931823492, 0.0022514297161251307, 0.0022510311100631952, 0.002250588731840253, 0.0022501933854073286, 0.0022497479803860188, 0.0022494180593639612, 0.0022490359842777252, 0.0022486320231109858, 0.002248307690024376, 0.0022479237522929907, 0.0022475740406662226, 0.002247236203402281, 0.0022468811366707087, 0.0022465267684310675, 0.0022462476044893265, 0.002245918847620487, 0.0022455723956227303, 0.0022452741395682096, 0.002244980074465275, 0.0022447044029831886, 0.002244387986138463, 0.0022441016044467688, 0.002243873430415988, 0.0022435872815549374, 0.002243351424112916, 0.002243097173050046, 0.002242857590317726, 0.002242554444819689, 0.002242309506982565, 0.002242080634459853, 0.0022418403532356024, 0.002241649432107806, 0.0022413895931094885, 0.0022411595564335585, 0.0022409523371607065, 0.002240734174847603, 0.0022405257914215326, 0.0022403413895517588, 0.002240119967609644, 0.0022398889996111393, 0.002239719033241272, 0.0022394810803234577, 0.00223930343054235, 0.002239132998511195, 0.00223891856148839, 0.0022388040088117123, 0.0022385825868695974, 0.0022383732721209526, 0.002238233806565404, 0.002238054294139147, 0.0022378703579306602, 0.0022377038840204477, 0.002237542299553752, 0.0022373907268047333, 0.0022372023668140173, 0.0022370831575244665, 0.0022369197104126215, 0.0022367846686393023, 0.0022366270422935486, 0.0022364945616573095, 0.002236350439488888, 0.0022361904848366976, 0.002236053580418229, 0.0022359145805239677, 0.0022357781417667866, 0.0022356545086950064, 0.0022355318069458008, 0.0022353895474225283, 0.0022352542728185654, 0.002235119929537177, 0.002234979998320341, 0.00223489198833704, 0.0022347536869347095, 0.0022346358746290207, 0.0022345150355249643, 0.0022343951277434826, 0.0022342782467603683, 0.0022341650910675526, 0.0022340717259794474, 0.002233965089544654, 0.0022338530980050564, 0.002233739709481597, 0.0022336505353450775, 0.0022335329558700323, 0.0022334272507578135, 0.002233340172097087, 0.0022332309745252132, 0.002233147853985429, 0.0022330463398247957, 0.0022329457569867373, 0.0022328472696244717, 0.0022327762562781572, 0.0022326570469886065, 0.002232583938166499, 0.0022324889432638884, 0.0022323988378047943, 0.002232304075732827, 0.0022322151344269514, 0.0022321450524032116, 0.0022320454008877277, 0.0022319615818560123, 0.0022319024428725243, 0.00223180721513927, 0.0022317369002848864, 0.002231638878583908, 0.002231568330898881, 0.0022314756643027067, 0.002231401624158025, 0.002231328748166561, 0.0022312572691589594, 0.0022311783395707607, 0.002231116406619549, 0.002231032121926546, 0.002230958081781864, 0.002230886835604906, 0.002230824902653694, 0.0022307420149445534, 0.0022306914906948805, 0.002230615820735693, 0.0022305487655103207, 0.002230490790680051, 0.002230419311672449, 0.002230351325124502, 0.0022302863653749228, 0.002230227692052722, 0.002230168553069234, 0.0022300954442471266, 0.0022300423588603735, 0.0022299792617559433, 0.0022299131378531456, 0.0022298649419099092, 0.0022298169787973166, 0.0022297417744994164, 0.0022296865936368704, 0.0022296446841210127, 0.0022295888047665358, 0.0022295196540653706, 0.00222946610301733, 0.0022294162772595882, 0.0022293610963970423, 0.0022293119691312313, 0.0022292505018413067, 0.002229200443252921, 0.0022291422355920076, 0.0022291007917374372, 0.002229048404842615, 0.0022290104534476995, 0.002228947589173913, 0.002228901954367757, 0.002228852827101946, 0.0022288041654974222, 0.0022287580650299788, 0.002228710101917386, 0.0022286653984338045, 0.0022286190651357174, 0.0022285727318376303, 0.00222854339517653, 0.002228489611297846, 0.0022284439764916897, 0.002228405326604843, 0.0022283592261373997, 0.0022283256985247135, 0.002228278899565339, 0.0022282369900494814, 0.002228192752227187, 0.0022281529381871223, 0.0022281159181147814, 0.0022280695848166943, 0.0022280323319137096, 0.0022279913537204266, 0.002227950841188431, 0.0022279100958257914, 0.0022278695832937956, 0.0022278374526649714, 0.002227799268439412, 0.0022277594543993473, 0.0022277242969721556, 0.002227682387456298, 0.0022276523523032665, 0.0022276141680777073, 0.0022275771480053663, 0.00222754362039268, 0.0022275082301348448, 0.0022274760995060205, 0.0022274483926594257, 0.0022274055518209934, 0.0022273731883615255, 0.0022273422218859196, 0.0022273024078458548, 0.0022272698115557432, 0.002227234886959195, 0.002227206015959382, 0.0022271755151450634, 0.0022271377965807915, 0.0022271135821938515, 0.0022270772606134415, 0.0022270451299846172, 0.0022270134650170803, 0.00222698668949306, 0.0022269580513238907, 0.0022269212640821934, 0.002226897282525897, 0.0022268653847277164, 0.0022268379107117653, 0.0022268141619861126, 0.002226777607575059, 0.0022267468739300966, 0.002226723125204444, 0.0022266951855272055, 0.00222667190246284, 0.002226640935987234, 0.0022266164887696505, 0.002226590644568205, 0.0022265620063990355, 0.0022265352308750153, 0.002226510550826788, 0.002226484240964055, 0.002226456766948104, 0.002226428361609578, 0.0022264006547629833, 0.00222638132981956, 0.002226356416940689, 0.002226330805569887, 0.0022263044957071543, 0.002226281212642789, 0.002226256299763918, 0.0022262302227318287, 0.002226205775514245, 0.0022261852864176035, 0.002226160606369376, 0.0022261349949985743, 0.002226118464022875, 0.0022260963451117277, 0.0022260716650635004, 0.0022260439582169056, 0.0022260260302573442, 0.0022259990219026804, 0.0022259799297899008, 0.0022259573452174664, 0.0022259410470724106, 0.0022259156685322523, 0.0022258954122662544, 0.0022258751560002565, 0.002225857926532626, 0.0022258323151618242, 0.00222581229172647, 0.0022257925011217594, 0.0022257722448557615, 0.0022257501259446144, 0.0022257291711866856, 0.0022257110103964806, 0.002225690521299839, 0.002225672360509634, 0.00222565489821136, 0.0022256337106227875, 0.002225613920018077, 0.002225595060735941, 0.0022255757357925177, 0.002225557342171669, 0.0022255408111959696, 0.0022255200892686844, 0.0022255026269704103, 0.0022254863288253546, 0.002225464442744851, 0.0022254465147852898, 0.0022254285868257284, 0.0022254125215113163, 0.0022253948263823986, 0.0022253775969147682, 0.0022253592032939196, 0.002225344069302082, 0.002225327305495739, 0.0022253128699958324, 0.002225293777883053, 0.002225276082754135, 0.002225258154794574, 0.0022252430208027363, 0.002225227188318968, 0.002225212287157774, 0.0022251957561820745, 0.002225178061053157, 0.002225163159891963, 0.002225147094577551, 0.0022251312620937824, 0.0022251163609325886, 0.00222510052844882, 0.00222508586011827, 0.002225069561973214, 0.002225056290626526, 0.0022250418551266193, 0.0022250227630138397, 0.0022250101901590824, 0.0022249955218285322, 0.0022249803878366947, 0.002224965253844857, 0.0022249503526836634, 0.0022249375469982624, 0.002224924275651574, 0.0022249086759984493, 0.0022248958703130484, 0.002224878640845418, 0.002224865835160017, 0.0022248527966439724, 0.002224839525297284, 0.0022248243913054466, 0.002224810654297471, 0.0022247990127652884, 0.0022247883025556803, 0.002224771771579981, 0.002224756870418787, 0.0022247449960559607, 0.002224731957539916, 0.0022247196175158024, 0.002224705647677183, 0.0022246947046369314, 0.0022246818989515305, 0.0022246669977903366, 0.0022246548905968666, 0.0022246430162340403, 0.0022246316075325012, 0.0022246178705245256, 0.002224608091637492, 0.002224594121798873, 0.0022245810832828283, 0.0022245696745812893, 0.0022245582658797503, 0.002224544994533062, 0.002224533585831523, 0.002224523341283202, 0.0022245103027671576, 0.0022244977299124002, 0.002224486554041505, 0.0022244765423238277, 0.002224462805315852, 0.002224452793598175, 0.002224442083388567, 0.0022244304418563843, 0.002224418567493558, 0.002224407158792019, 0.00222439575009048, 0.0022243850398808718, 0.0022243750281631947, 0.002224364085122943, 0.002224353142082691, 0.0022243415005505085, 0.002224333118647337, 0.002224318915978074, 0.0022243084385991096, 0.0022243012208491564, 0.0022242898121476173, 0.002224277937784791, 0.0022242660634219646, 0.002224255120381713, 0.002224243711680174, 0.002224236261099577, 0.0022242222912609577, 0.002224209951236844, 0.002224198542535305, 0.002224189694970846, 0.0022241780534386635, 0.0022241680417209864, 0.002224152674898505, 0.0022241449914872646, 0.0022241314873099327, 0.002224122639745474, 0.00222410773858428, 0.0022240958642214537, 0.002224087016656995, 0.002224074909463525, 0.0022240576799958944, 0.002224051859229803, 0.0022240360267460346, 0.002224019030109048, 0.0022239950485527515, 0.0022239782847464085, 0.0022239708341658115, 0.00222395290620625, 0.0022239265963435173, 0.0022238895762711763, 0.0022238539531826973, 0.002223796909675002, 0.0022237496450543404, 0.0022236814256757498, 0.0022235678043216467, 0.002223559422418475, 0.002223178744316101, 0.002224364783614874, 0.002222228329628706, 0.0022185209672898054, 0.0047942292876541615, 0.05542019009590149, 0.05594765022397041, 0.037310823798179626, 0.02687719464302063, 0.016426680609583855, 0.010592086240649223, 0.007096500601619482, 0.005625413730740547, 0.004508930258452892, 0.0037798346020281315, 0.0035208826884627342, 0.0032851235009729862, 0.0031551660504192114, 0.003091213759034872, 0.003030965104699135, 0.003002034267410636, 0.0029752084519714117, 0.002885011490434408, 0.002746961312368512, 0.002422124845907092, 0.0023010149598121643, 0.0022070796694606543, 0.0021675238385796547, 0.0021440538112074137, 0.0021357873920351267, 0.0021267884876579046, 0.0021209155675023794, 0.0021171614062041044, 0.00211386289447546, 0.002110925270244479, 0.002109293593093753, 0.00210760859772563, 0.0021061114966869354, 0.0021048719063401222, 0.002103806007653475, 0.002102914499118924, 0.0021020895801484585, 0.002101326361298561, 0.0021006898023188114, 0.0021000325214117765, 0.002099452307447791, 0.002098889322951436, 0.002098364755511284, 0.0020978597458451986, 0.0020974185317754745, 0.0020969905890524387, 0.00209657265804708, 0.002096188487485051, 0.0020958436653017998, 0.0020954697392880917, 0.002095157513394952, 0.0020948266610503197, 0.002094531897455454, 0.0020942427217960358, 0.0020939745008945465, 0.0020937221124768257, 0.002093464834615588, 0.0020932203624397516, 0.002092986600473523, 0.0020927658770233393, 0.002092574955895543, 0.0020923535339534283, 0.002092160051688552, 0.0020919591188430786, 0.002091767033562064, 0.0020916045177727938, 0.0020914163906127214, 0.002091262023895979, 0.002091089030727744, 0.0020909300073981285, 0.0020907933358103037, 0.00209063314832747, 0.002090492518618703, 0.0020903523545712233, 0.00209022406488657, 0.0020901018287986517, 0.00208998192101717, 0.002089868998154998, 0.002089755143970251, 0.002089644316583872, 0.002089535817503929, 0.0020894359331578016, 0.0020893330220133066, 0.0020892389584332705, 0.002089145127683878, 0.002089048270136118, 0.002088959328830242, 0.0020888743456453085, 0.0020887881983071566, 0.0020887067075818777, 0.0020886193960905075, 0.002088547218590975, 0.0020884699188172817, 0.0020883940160274506, 0.0020883281249552965, 0.002088253851979971, 0.0020881900563836098, 0.002088119275867939, 0.0020880524534732103, 0.002087991451844573, 0.0020879292860627174, 0.002087869681417942, 0.0020878114737570286, 0.0020877502392977476, 0.002087694825604558, 0.002087639644742012, 0.0020875888876616955, 0.0020875362679362297, 0.0020874841138720512, 0.002087433822453022, 0.002087387489154935, 0.002087336964905262, 0.0020872897002846003, 0.0020872412715107203, 0.002087199594825506, 0.0020871523302048445, 0.002087110187858343, 0.002087063156068325, 0.0020870224107056856, 0.0020869795698672533, 0.0020869416184723377, 0.0020869006402790546, 0.002086859894916415, 0.0020868240389972925, 0.0020867844577878714, 0.0020867502316832542, 0.0020867104176431894, 0.0020866768900305033, 0.0020866412669420242, 0.0020866096019744873, 0.0020865737460553646, 0.0020865402184426785, 0.0020865078549832106, 0.0020864754915237427, 0.0020864447578787804, 0.0020864149555563927, 0.002086382359266281, 0.0020863532554358244, 0.0020863257814198732, 0.00208629434928298, 0.0020862685050815344, 0.0020862389355897903, 0.0020862119272351265, 0.002086182124912739, 0.002086156979203224, 0.0020861313678324223, 0.0020861029624938965, 0.002086077816784382, 0.0020860524382442236, 0.0020860310178250074, 0.002086002379655838, 0.002085978165268898, 0.002085954649373889, 0.002085930434986949, 0.0020859085489064455, 0.002085884101688862, 0.0020858612842857838, 0.0020858391653746367, 0.002085817512124777, 0.002085795858874917, 0.0020857746712863445, 0.0020857544150203466, 0.002085733460262418, 0.002085713902488351, 0.0020856934133917093, 0.0020856717601418495, 0.002085653133690357, 0.0020856326445937157, 0.002085614250972867, 0.0020855951588600874, 0.0020855767652392387, 0.0020855593029409647, 0.0020855418406426907, 0.0020855257753282785, 0.002085506683215499, 0.002085489919409156, 0.0020854740869253874, 0.0020854563917964697, 0.002085440792143345, 0.002085423097014427, 0.0020854074973613024, 0.0020853914320468903, 0.0020853758323937654, 0.0020853609312325716, 0.0020853455644100904, 0.0020853301975876093, 0.0020853145979344845, 0.002085299463942647, 0.002085284097120166, 0.002085269894450903, 0.0020852559246122837, 0.0020852419547736645, 0.0020852277521044016, 0.002085213316604495, 0.0020852000452578068, 0.0020851860754191875, 0.0020851725712418556, 0.002085159532725811, 0.0020851455628871918, 0.0020851336885243654, 0.002085121115669608, 0.002085107145830989, 0.0020850948058068752, 0.0020850820001214743, 0.0020850698929280043, 0.0020850575529038906, 0.0020850461442023516, 0.0020850335713475943, 0.0020850221626460552, 0.0020850100554525852, 0.0020849991124123335, 0.002084987470880151, 0.002084976527839899, 0.0020849653519690037, 0.0020849548745900393, 0.002084944862872362, 0.0020849339198321104, 0.0020849229767918587, 0.0020849118009209633, 0.002084901789203286, 0.002084891777485609, 0.0020848813001066446, 0.002084871754050255, 0.0020848605781793594, 0.0020848510321229696, 0.002084841253235936, 0.0020848324056714773, 0.0020848221611231565, 0.0020848133135586977, 0.0020848035346716642, 0.002084794919937849, 0.0020847858395427465, 0.0020847769919782877, 0.0020847683772444725, 0.0020847581326961517, 0.00208474975079298, 0.002084741136059165, 0.0020847327541559935, 0.0020847248379141092, 0.0020847152918577194, 0.002084707608446479, 0.0020846989937126637, 0.0020846908446401358, 0.0020846824627369642, 0.002084673848003149, 0.0020846666302531958, 0.002084659179672599, 0.0020846505649387836, 0.002084642881527543, 0.0020846351981163025, 0.0020846277475357056, 0.0020846198312938213, 0.0020846128463745117, 0.00208460446447134, 0.002084596548229456, 0.0020845895633101463, 0.0020845818798989058, 0.002084574894979596, 0.0020845679100602865, 0.002084560226649046, 0.0020845537073910236, 0.002084547420963645, 0.0020845411345362663, 0.0020845336839556694, 0.002084527863189578, 0.0020845201797783375, 0.002084513194859028, 0.002084506442770362, 0.0020845013204962015, 0.0020844941027462482, 0.002084487583488226, 0.00208448083139956, 0.0020844745449721813, 0.0020844689570367336, 0.002084462670609355, 0.0020844561513513327, 0.0020844500977545977, 0.0020844440441578627, 0.0020844382233917713, 0.002084431704133749, 0.0020844258833676577, 0.0020844198297709227, 0.0020844137761741877, 0.00208440818823874, 0.0020844026003032923, 0.0020843972451984882, 0.0020843909587711096, 0.0020843856036663055, 0.0020843802485615015, 0.00208437442779541, 0.0020843693055212498, 0.0020843641832470894, 0.0020843588281422853, 0.0020843539386987686, 0.002084349049255252, 0.0020843432284891605, 0.002084338106215, 0.0020843325182795525, 0.0020843278616666794, 0.0020843232050538063, 0.0020843190141022205, 0.002084313193336129, 0.002084308071061969, 0.0020843034144490957, 0.0020842982921749353, 0.0020842934027314186, 0.002084288978949189, 0.0020842845551669598, 0.002084279665723443, 0.00208427500911057, 0.002084270352497697, 0.00208426546305418, 0.002084260806441307, 0.0020842563826590776, 0.002084251493215561, 0.002084247302263975, 0.0020842428784817457, 0.0020842382218688726, 0.0020842337980866432, 0.0020842289086431265, 0.002084225183352828, 0.0020842207595705986, 0.002084216335788369, 0.002084212377667427, 0.0020842074882239103, 0.0020842032972723246, 0.0020841998048126698, 0.002084195613861084, 0.002084191422909498, 0.002084187464788556, 0.0020841839723289013, 0.0020841797813773155, 0.002084176056087017, 0.0020841716323047876, 0.002084167907014489, 0.0020841641817241907, 0.0020841604564338923, 0.0020841569639742374, 0.0020841527730226517, 0.0020841495133936405, 0.0020841455552726984, 0.0020841422956436872, 0.0020841381046921015, 0.002084134379401803, 0.0020841313526034355, 0.0020841271616518497, 0.002084123669192195, 0.002084120409563184, 0.0020841171499341726, 0.002084113657474518, 0.0020841096993535757, 0.002084106905385852, 0.0020841029472649097, 0.0020840992219746113, 0.0020840959623456, 0.0020840931683778763, 0.002084089210256934, 0.002084085950627923, 0.0020840829238295555, 0.0020840794313699007, 0.0020840757060796022, 0.002084073144942522, 0.0020840701181441545, 0.002084066392853856, 0.0020840629003942013, 0.0020840598735958338, 0.0020840568467974663, 0.0020840538199990988, 0.0020840507932007313, 0.002084047067910433, 0.0020840445067733526, 0.0020840419456362724, 0.0020840379875153303, 0.002084034960716963, 0.0020840319339185953, 0.002084028674289584, 0.0020840258803218603, 0.0020840230863541365, 0.002084020059555769, 0.002084017265588045, 0.0020840142387896776, 0.00208401121199131, 0.00208400865085423, 0.0020840056240558624, 0.0020840030629187822, 0.002083999803289771, 0.002083997242152691, 0.002083993749693036, 0.0020839914213865995, 0.002083988394588232, 0.002083985600620508, 0.0020839832723140717, 0.002083980245515704, 0.002083977684378624, 0.0020839748904109, 0.0020839718636125326, 0.0020839690696448088, 0.0020839660428464413, 0.0020839637145400047, 0.002083960920572281, 0.002083958825096488, 0.002083956031128764, 0.002083953469991684, 0.0020839509088546038, 0.00208394811488688, 0.002083945320919156, 0.0020839429926127195, 0.0020839404314756393, 0.002083938103169203, 0.0020839355420321226, 0.0020839329808950424, 0.002083930652588606, 0.0020839280914515257, 0.002083925995975733, 0.002083923202008009, 0.0020839213393628597, 0.002083918545395136, 0.0020839166827499866, 0.0020839141216129065, 0.0020839115604758263, 0.0020839092321693897, 0.002083907136693597, 0.0020839048083871603, 0.0020839024800807238, 0.0020838999189436436, 0.0020838980562984943, 0.002083895727992058, 0.0020838933996856213]\n",
      "[0.1964174062013626, 0.16987952589988708, 0.15680181980133057, 0.1472945660352707, 0.13671308755874634, 0.12547945976257324, 0.11607533693313599, 0.10535410791635513, 0.09927025437355042, 0.09328093379735947, 0.08941935747861862, 0.08770088106393814, 0.08319145441055298, 0.07877390831708908, 0.07604745030403137, 0.07354318350553513, 0.06848940253257751, 0.06723903119564056, 0.06324891746044159, 0.05980832502245903, 0.05902940779924393, 0.05513133108615875, 0.0560368187725544, 0.051164254546165466, 0.048406343907117844, 0.04795900359749794, 0.04677627235651016, 0.04467446357011795, 0.043290235102176666, 0.0418669730424881, 0.040881089866161346, 0.040673915296792984, 0.03994905203580856, 0.03940775990486145, 0.03784589096903801, 0.03769402951002121, 0.037369802594184875, 0.03586970269680023, 0.03600506857037544, 0.0348404161632061, 0.03500441461801529, 0.034014806151390076, 0.033447667956352234, 0.03298896551132202, 0.03308248147368431, 0.0325007289648056, 0.03231838718056679, 0.03171563521027565, 0.03162418305873871, 0.03231053426861763, 0.03115599788725376, 0.0320235975086689, 0.030970310792326927, 0.03173817694187164, 0.030381543561816216, 0.0310591459274292, 0.030501002445816994, 0.030855176970362663, 0.030312173068523407, 0.030262047424912453, 0.02980811521410942, 0.029941702261567116, 0.030283767729997635, 0.029201796278357506, 0.029823563992977142, 0.029752984642982483, 0.02972753718495369, 0.02930355817079544, 0.02955302782356739, 0.029456861317157745, 0.029606161639094353, 0.02944948896765709, 0.02942747063934803, 0.029225902631878853, 0.029398689046502113, 0.029341263696551323, 0.02932579815387726, 0.02935701422393322, 0.029128840193152428, 0.02946530655026436, 0.02950824424624443, 0.03003855235874653, 0.030329082161188126, 0.030112648382782936, 0.029695995151996613, 0.02978038787841797, 0.029920067638158798, 0.029892148450016975, 0.030328966677188873, 0.03075096197426319, 0.030372021719813347, 0.03085118718445301, 0.02994905412197113, 0.02989872731268406, 0.029826462268829346, 0.030022425577044487, 0.029955847188830376, 0.030244307592511177, 0.030414646491408348, 0.030180830508470535, 0.030151953920722008, 0.03018871694803238, 0.0301500391215086, 0.02996743470430374, 0.03027801588177681, 0.030316922813653946, 0.030105866491794586, 0.03014872595667839, 0.0300369244068861, 0.030114848166704178, 0.030240900814533234, 0.030319292098283768, 0.030221376568078995, 0.030378445982933044, 0.030111616477370262, 0.030696740373969078, 0.030534617602825165, 0.029886120930314064, 0.0301282349973917, 0.030187804251909256, 0.029896125197410583, 0.02990749105811119, 0.03000653348863125, 0.029526732861995697, 0.02970186062157154, 0.030102822929620743, 0.03011828474700451, 0.029938247054815292, 0.029936622828245163, 0.030097469687461853, 0.03009282611310482, 0.030140230432152748, 0.030164361000061035, 0.03017852082848549, 0.030119217932224274, 0.030072417110204697, 0.030215945094823837, 0.03023703768849373, 0.030175982043147087, 0.030251555144786835, 0.030291879549622536, 0.030260823667049408, 0.03020014800131321, 0.030272549018263817, 0.030335690826177597, 0.030363034456968307, 0.03033333830535412, 0.030265366658568382, 0.030311841517686844, 0.030378220602869987, 0.030337736010551453, 0.03034442476928234, 0.030376756563782692, 0.03041898086667061, 0.03045041486620903, 0.030523989349603653, 0.030451519414782524, 0.030456263571977615, 0.030455827713012695, 0.03044319339096546, 0.03045855276286602, 0.030510760843753815, 0.030512891709804535, 0.030562326312065125, 0.03055974468588829, 0.030559180304408073, 0.030564432963728905, 0.030588731169700623, 0.030516834929585457, 0.030560005456209183, 0.03066400997340679, 0.03056650608778, 0.03054901398718357, 0.03058464825153351, 0.030628154054284096, 0.030613088980317116, 0.030621495097875595, 0.03059590794146061, 0.030617456883192062, 0.030657537281513214, 0.030623264610767365, 0.030659792944788933, 0.03066546842455864, 0.03065558150410652, 0.030691085383296013, 0.030675657093524933, 0.030705833807587624, 0.030726516619324684, 0.0307005625218153, 0.03072870709002018, 0.03072321228682995, 0.03073647990822792, 0.030788244679570198, 0.030810844153165817, 0.0307910293340683, 0.030789228156208992, 0.030792539939284325, 0.030820751562714577, 0.030856270343065262, 0.03083634562790394, 0.03085491806268692, 0.030871711671352386, 0.030866410583257675, 0.030820021405816078, 0.030810365453362465, 0.03087564744055271, 0.030835073441267014, 0.030826902016997337, 0.03088488057255745, 0.030869867652654648, 0.030856063589453697, 0.030851012095808983, 0.030856627970933914, 0.03086959756910801, 0.030911562964320183, 0.030895855277776718, 0.030886847525835037, 0.03089115396142006, 0.0309001374989748, 0.03092910535633564, 0.030929546803236008, 0.030988043174147606, 0.030989758670330048, 0.03097165748476982, 0.031007986515760422, 0.031016359105706215, 0.030990416184067726, 0.031013691797852516, 0.031062807887792587, 0.031035559251904488, 0.03098558448255062, 0.03100060671567917, 0.03102754056453705, 0.031034599989652634, 0.03101218305528164, 0.03102431260049343, 0.031077714636921883, 0.03108784928917885, 0.031052658334374428, 0.03102993220090866, 0.031049543991684914, 0.031103475019335747, 0.031091654673218727, 0.03111688420176506, 0.0311368890106678, 0.031132027506828308, 0.031097861006855965, 0.031045060604810715, 0.031101757660508156, 0.031140180304646492, 0.031150057911872864, 0.031140610575675964, 0.03113115392625332, 0.03110656514763832, 0.03110557235777378, 0.031105563044548035, 0.031116044148802757, 0.031110338866710663, 0.031155850738286972, 0.031159184873104095, 0.031117038801312447, 0.031143581494688988, 0.031149087473750114, 0.03114151395857334, 0.031156962737441063, 0.031165175139904022, 0.031145339831709862, 0.03115653246641159, 0.03117191232740879, 0.031163372099399567, 0.03113366663455963, 0.031154822558164597, 0.031197937205433846, 0.031233198940753937, 0.031149351969361305, 0.031120164319872856, 0.03116564266383648, 0.031207967549562454, 0.03121272101998329, 0.031205639243125916, 0.0312051959335804, 0.031219491735100746, 0.031202485784888268, 0.031217491254210472, 0.031186824664473534, 0.03120362013578415, 0.031235121190547943, 0.031250450760126114, 0.031241409480571747, 0.03122544102370739, 0.031213337555527687, 0.031249063089489937, 0.03123112954199314, 0.031226428225636482, 0.031271208077669144, 0.031287189573049545, 0.03129184618592262, 0.03129586949944496, 0.03128332272171974, 0.03128742426633835, 0.03129781037569046, 0.031295113265514374, 0.031291648745536804, 0.03130834177136421, 0.03128360956907272, 0.03127865493297577, 0.031312521547079086, 0.0313444584608078, 0.031347617506980896, 0.03132067248225212, 0.03134162351489067, 0.031340550631284714, 0.03130612522363663, 0.03129231184720993, 0.031354378908872604, 0.03138868510723114, 0.03138848394155502, 0.03135289251804352, 0.0313643254339695, 0.031382426619529724, 0.03135770931839943, 0.031363461166620255, 0.03137882053852081, 0.03137265145778656, 0.031376637518405914, 0.03137407451868057, 0.031380925327539444, 0.03139624372124672, 0.03142274543642998, 0.03139330819249153, 0.03137261047959328, 0.03138585016131401, 0.03141850233078003, 0.03141896426677704, 0.03141520917415619, 0.031416215002536774, 0.03143777698278427, 0.031445037573575974, 0.031428586691617966, 0.03144053369760513, 0.031444139778614044, 0.031457167118787766, 0.03144209086894989, 0.03143984451889992, 0.03141823783516884, 0.03140665963292122, 0.03143198788166046, 0.03147601708769798, 0.031476862728595734, 0.031464673578739166, 0.03144893795251846, 0.03145100176334381, 0.03146809712052345, 0.03148657828569412, 0.03146626427769661, 0.03143931180238724, 0.03145736828446388, 0.03147181123495102, 0.03150056675076485, 0.031523384153842926, 0.03151446953415871, 0.03150869160890579, 0.031508203595876694, 0.03152655065059662, 0.03151153773069382, 0.031510867178440094, 0.0314946174621582, 0.03152753412723541, 0.031537897884845734, 0.031547680497169495, 0.03153179958462715, 0.03152503818273544, 0.03153226524591446, 0.03154762089252472, 0.03154623880982399, 0.031519290059804916, 0.031537268310785294, 0.03156318888068199, 0.03155571594834328, 0.031561464071273804, 0.03156062960624695, 0.03156290948390961, 0.03156692907214165, 0.03156767413020134, 0.031534649431705475, 0.03153073042631149, 0.03155175969004631, 0.031544264405965805, 0.03155316412448883, 0.0315619632601738, 0.031571611762046814, 0.03157635033130646, 0.0315643846988678, 0.03154812753200531, 0.031548045575618744, 0.03159667178988457, 0.031596262007951736, 0.03157908469438553, 0.031560350209474564, 0.03157808631658554, 0.031576644629240036, 0.03157161548733711, 0.03157540410757065, 0.031586192548274994, 0.03160214424133301, 0.031594060361385345, 0.031582292169332504, 0.0315985232591629, 0.03160581365227699, 0.03159914165735245, 0.03163127228617668, 0.03163953125476837, 0.031619179993867874, 0.03161682188510895, 0.031631868332624435, 0.031622275710105896, 0.03160052374005318, 0.03161928430199623, 0.031617581844329834, 0.031619418412446976, 0.03161081671714783, 0.03162775933742523, 0.031631242483854294, 0.03162781894207001, 0.031631190329790115, 0.031630028039216995, 0.03165404498577118, 0.03164870664477348, 0.031641848385334015, 0.0316416397690773, 0.0316552072763443, 0.03167425096035004, 0.03165027126669884, 0.03165554255247116, 0.03165154159069061, 0.031650710850954056, 0.03165866807103157, 0.03166067972779274, 0.0316627062857151, 0.03165040910243988, 0.031652044504880905, 0.03166194260120392, 0.03165985271334648, 0.0316617526113987, 0.031656648963689804, 0.03167589008808136, 0.031673576682806015, 0.03164234384894371, 0.031659532338380814, 0.03166014701128006, 0.0316876657307148, 0.031685683876276016, 0.0316622294485569, 0.03165720775723457, 0.03165366128087044, 0.03165136277675629, 0.031662143766880035, 0.03168554604053497, 0.031699832528829575, 0.03168782219290733, 0.031667958945035934, 0.031672462821006775, 0.03170111030340195, 0.031683094799518585, 0.0316791869699955, 0.031661506742239, 0.03167539834976196, 0.031675830483436584, 0.03170112147927284, 0.03170255944132805, 0.03170540928840637, 0.03171957656741142, 0.031717587262392044, 0.031722091138362885, 0.03170756250619888, 0.03169410303235054, 0.03170224279165268, 0.0317067950963974, 0.03172960877418518, 0.031735632568597794, 0.031762830913066864, 0.03176450356841087, 0.0317264199256897, 0.031714290380477905, 0.03172868490219116, 0.0317436158657074, 0.031731974333524704, 0.03173096105456352, 0.03173110634088516, 0.03172910585999489, 0.031752362847328186, 0.031744200736284256, 0.03173372521996498, 0.03173832595348358, 0.0317477323114872, 0.031757183372974396, 0.031773194670677185, 0.03174837678670883, 0.03175738826394081, 0.03178143873810768, 0.03178388252854347, 0.03176772966980934, 0.031764887273311615, 0.03178421035408974, 0.0317721925675869, 0.03176362067461014, 0.031765472143888474, 0.03178661689162254, 0.031762391328811646, 0.03176933526992798, 0.03177528455853462, 0.031778834760189056, 0.03178096190094948, 0.03178665041923523, 0.031792450696229935, 0.031790636479854584, 0.031786397099494934, 0.031785596162080765, 0.03180401027202606, 0.03178994730114937, 0.03178088739514351, 0.03179386630654335, 0.031807444989681244, 0.03182957321405411, 0.031802698969841, 0.03179531544446945, 0.03179411217570305, 0.031792450696229935, 0.03180163726210594, 0.031800031661987305, 0.03182094171643257, 0.031837381422519684, 0.031813375651836395, 0.031804896891117096, 0.03181834518909454, 0.03181617334485054, 0.031822096556425095, 0.031808651983737946, 0.031821176409721375, 0.03182104974985123, 0.031818877905607224, 0.03180662542581558, 0.03180951252579689, 0.03182573616504669, 0.03184616565704346, 0.03182037174701691, 0.031811799854040146, 0.03184302896261215, 0.03184818476438522, 0.03185117617249489, 0.03184531629085541, 0.031854961067438126, 0.031870286911726, 0.031850315630435944, 0.031838592141866684, 0.03184055909514427, 0.03186965361237526, 0.031880371272563934, 0.03186352178454399, 0.0318678580224514, 0.03185800462961197, 0.03186435624957085, 0.03186587244272232, 0.03188060596585274, 0.031834665685892105, 0.031831804662942886, 0.03186802938580513, 0.0317796915769577, 0.03166285157203674, 0.031453970819711685, 0.03169947490096092, 0.057622410356998444, 0.08827314525842667, 0.06871340423822403, 0.05119879171252251, 0.04585087671875954, 0.03791900351643562, 0.03533821552991867, 0.032704394310712814, 0.03210423141717911, 0.03275282308459282, 0.03207753598690033, 0.031578484922647476, 0.031270746141672134, 0.03104628063738346, 0.031529150903224945, 0.031959325075149536, 0.0319858193397522, 0.03182368725538254, 0.03163836896419525, 0.03151296079158783, 0.031441375613212585, 0.030268710106611252, 0.030456680804491043, 0.030893344432115555, 0.031039398163557053, 0.031130367890000343, 0.031121764332056046, 0.03115350753068924, 0.03125125542283058, 0.03131725639104843, 0.03139067813754082, 0.03145025670528412, 0.03148234263062477, 0.03152044117450714, 0.031552448868751526, 0.031584035605192184, 0.03161071613430977, 0.0316324420273304, 0.03166936710476875, 0.0317075289785862, 0.03173752874135971, 0.031758565455675125, 0.03180398792028427, 0.03182515501976013, 0.031842853873968124, 0.03186086192727089, 0.03186747059226036, 0.031898755580186844, 0.03192196041345596, 0.03193794563412666, 0.03196585178375244, 0.031991541385650635, 0.032010916620492935, 0.03202798217535019, 0.032051194459199905, 0.03207173943519592, 0.032088205218315125, 0.03210676833987236, 0.03212369978427887, 0.032136231660842896, 0.03214682638645172, 0.03214805945754051, 0.0321660079061985, 0.032187432050704956, 0.03221423923969269, 0.03222975879907608, 0.03224095329642296, 0.03224574029445648, 0.03224702179431915, 0.032261986285448074, 0.032272618263959885, 0.03227444738149643, 0.032274577766656876, 0.03228408843278885, 0.03230120986700058, 0.032312486320734024, 0.03232378512620926, 0.032335370779037476, 0.03233485296368599, 0.03233308345079422, 0.03234400227665901, 0.0323515459895134, 0.03236415237188339, 0.03237108886241913, 0.03238263726234436, 0.03238961100578308, 0.0323944054543972, 0.03238959237933159, 0.0323900431394577, 0.03238798677921295, 0.032395776361227036, 0.03240690380334854, 0.03241792321205139, 0.03242533281445503, 0.03243926912546158, 0.03244749456644058, 0.03245464339852333, 0.03245951235294342, 0.032458771020174026, 0.03245892375707626, 0.03245411813259125, 0.03244949132204056, 0.03245754539966583, 0.032462283968925476, 0.0324760265648365, 0.032485231757164, 0.03249608725309372, 0.03250255063176155, 0.03250687196850777, 0.03251151740550995, 0.03251901641488075, 0.0325181670486927, 0.032517869025468826, 0.03252340853214264, 0.032528650015592575, 0.032532911747694016, 0.03253002464771271, 0.032530542463064194, 0.03253491222858429, 0.03253908082842827, 0.03254450112581253, 0.032547034323215485, 0.03255239874124527, 0.03255161643028259, 0.03255302831530571, 0.032552462071180344, 0.03255355730652809, 0.032558027654886246, 0.03256130591034889, 0.03256569802761078, 0.03256736695766449, 0.03257475420832634, 0.03258140757679939, 0.032577741891145706, 0.032580312341451645, 0.0325825959444046, 0.03258715197443962, 0.032590173184871674, 0.03259383141994476, 0.03259442001581192, 0.03259527310729027, 0.032591525465250015, 0.032587386667728424, 0.03258492052555084, 0.03258726745843887, 0.03258093073964119, 0.032577574253082275, 0.03258135914802551, 0.032587915658950806, 0.03258958458900452, 0.032595232129096985, 0.0326041616499424, 0.03261119872331619, 0.032617583870887756, 0.03262225538492203, 0.03262738510966301, 0.03263002261519432, 0.03263000398874283, 0.03263518959283829, 0.03263213485479355, 0.032633163034915924, 0.032636359333992004, 0.03263794258236885, 0.03263990581035614, 0.03264221176505089, 0.0326455682516098, 0.03264855220913887, 0.03265335410833359, 0.03265601769089699, 0.03266230970621109, 0.03266545385122299, 0.032670389860868454, 0.03266860172152519, 0.032668426632881165, 0.03266966715455055, 0.03267190605401993, 0.03267137333750725, 0.032671693712472916, 0.03267218545079231, 0.032672006636857986, 0.03267746791243553, 0.032676439732313156, 0.032674435526132584, 0.03267529234290123, 0.03267238661646843, 0.03266898915171623, 0.03266836702823639, 0.03267347440123558, 0.0326741598546505, 0.0326765738427639, 0.03268065303564072, 0.03268207609653473, 0.032680995762348175, 0.03268614038825035, 0.03268905729055405, 0.032693807035684586, 0.032693278044462204, 0.032694071531295776, 0.03269096091389656, 0.03269167244434357, 0.032695066183805466, 0.03269670531153679, 0.03269844129681587, 0.032695118337869644, 0.03269388899207115, 0.032690972089767456, 0.03268684074282646, 0.03268978372216225, 0.03269754350185394, 0.03270559757947922, 0.032710831612348557, 0.03271310403943062, 0.032714150846004486, 0.032716866582632065, 0.032716915011405945, 0.03271574527025223, 0.032715704292058945, 0.032719362527132034, 0.03272758424282074, 0.032732974737882614, 0.03273474797606468, 0.03273146227002144, 0.03273335099220276, 0.03273749724030495, 0.032734353095293045, 0.03273535892367363, 0.03274150192737579, 0.032739073038101196, 0.03274238482117653, 0.03274613991379738, 0.0327453687787056, 0.03274724632501602, 0.032747309654951096, 0.03274152800440788, 0.032737381756305695, 0.032740265130996704, 0.032743629068136215, 0.03274175152182579, 0.032743219286203384, 0.032738909125328064, 0.03274114429950714, 0.03274259716272354, 0.03273969516158104, 0.03273408114910126, 0.03273415565490723, 0.03273875266313553, 0.032741401344537735, 0.032742470502853394, 0.03274664282798767, 0.03275248780846596, 0.03275357931852341, 0.03276176378130913, 0.032766059041023254, 0.03276835009455681, 0.03276846557855606, 0.032767411321401596, 0.03276539593935013, 0.03276611492037773, 0.03276608884334564, 0.032768238335847855, 0.0327698178589344, 0.03277256712317467, 0.032773375511169434, 0.032775238156318665, 0.03277643397450447, 0.03277810662984848, 0.03277929127216339, 0.03277645632624626, 0.032777704298496246, 0.03277253732085228, 0.03277657553553581, 0.03277995064854622, 0.03277995437383652, 0.03277909383177757, 0.032782312482595444, 0.03278321772813797, 0.032782312482595444, 0.03277866914868355, 0.032782115042209625, 0.032781049609184265, 0.03278074041008949, 0.032780878245830536, 0.032780908048152924, 0.03278149664402008, 0.03277941793203354, 0.03278057277202606, 0.03278397023677826, 0.03278254717588425, 0.03278248757123947, 0.032778989523649216, 0.03278065845370293, 0.03278200700879097, 0.03278437629342079, 0.03278403356671333, 0.03278443589806557, 0.03278883174061775, 0.032790809869766235, 0.03278925269842148, 0.03279636800289154, 0.03280076012015343, 0.032801952213048935, 0.03280552476644516, 0.032804589718580246, 0.03280216082930565, 0.03279681131243706, 0.0327957384288311, 0.03279433771967888, 0.03279387578368187, 0.032794054597616196, 0.03279338777065277, 0.03279478847980499, 0.03279353678226471, 0.03279474377632141, 0.03279561549425125, 0.03279753029346466, 0.03280065953731537, 0.03280593454837799, 0.03280629590153694, 0.032804764807224274, 0.03280961886048317, 0.03281235322356224, 0.03280903026461601, 0.03280426561832428, 0.03280159831047058, 0.032799623906612396, 0.032791927456855774, 0.03278670832514763, 0.032782599329948425, 0.032783471047878265, 0.032784782350063324, 0.03278672322630882, 0.03279056400060654, 0.03279231861233711, 0.032794080674648285, 0.03279639407992363, 0.032796405255794525, 0.032795537263154984, 0.03279535472393036, 0.03279455751180649, 0.03279530629515648, 0.03279747813940048, 0.032799724489450455, 0.032801199704408646, 0.032803989946842194, 0.032806459814310074, 0.032807156443595886, 0.032807912677526474, 0.03280680999159813, 0.03280666097998619, 0.03280758857727051, 0.03280939906835556, 0.032804541289806366, 0.03280724957585335, 0.03280353173613548, 0.032803330570459366, 0.032803937792778015, 0.032803554087877274, 0.032805293798446655, 0.0328066386282444, 0.03280624747276306, 0.032805707305669785, 0.03280641883611679, 0.032813262194395065, 0.03281434625387192, 0.03281703591346741, 0.03281829506158829, 0.032819028943777084, 0.03281942009925842, 0.03281978517770767, 0.03281985968351364, 0.03281784430146217, 0.03281313553452492, 0.032810427248477936, 0.03281155228614807, 0.03281350061297417, 0.0328185074031353, 0.03281933441758156, 0.03282003477215767, 0.03282199054956436, 0.03282760828733444, 0.03282729163765907, 0.03282685577869415, 0.032829929143190384, 0.03282558545470238, 0.03282369300723076, 0.032823480665683746, 0.03282332792878151, 0.03282808139920235, 0.032829947769641876, 0.032836493104696274, 0.032836686819791794, 0.032840944826602936, 0.03284405544400215, 0.03284408897161484, 0.03284527361392975, 0.032846540212631226, 0.032841362059116364, 0.03283458203077316, 0.032833926379680634, 0.03282983973622322, 0.03283313661813736, 0.03283216059207916, 0.03283248841762543, 0.032838642597198486, 0.0328412763774395, 0.03284356743097305, 0.03284166008234024, 0.032840173691511154, 0.032843973487615585, 0.032844044268131256, 0.03284439817070961, 0.03284287825226784, 0.03284304961562157, 0.032840095460414886, 0.03283679857850075, 0.03283604234457016, 0.032835230231285095, 0.0328366756439209, 0.032838039100170135, 0.03283824399113655, 0.0328395776450634, 0.032844752073287964, 0.03284880146384239, 0.03284960985183716, 0.03284843638539314, 0.03284817188978195, 0.032845791429281235, 0.032848604023456573, 0.03284815698862076]\n",
      "[0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019999999552965164, 0.03999999910593033, 0.05999999865889549, 0.30000001192092896, 0.1599999964237213, 0.1599999964237213, 0.18000000715255737, 0.18000000715255737, 0.18000000715255737, 0.20000000298023224, 0.25999999046325684, 0.14000000059604645, 0.2199999988079071, 0.20000000298023224, 0.1599999964237213, 0.18000000715255737, 0.2800000011920929, 0.20000000298023224, 0.25999999046325684, 0.25999999046325684, 0.2199999988079071, 0.2800000011920929, 0.2199999988079071, 0.2199999988079071, 0.20000000298023224, 0.23999999463558197, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.2199999988079071, 0.2199999988079071, 0.2199999988079071, 0.2199999988079071, 0.20000000298023224, 0.23999999463558197, 0.2199999988079071, 0.23999999463558197, 0.18000000715255737, 0.23999999463558197, 0.2199999988079071, 0.23999999463558197, 0.20000000298023224, 0.23999999463558197, 0.23999999463558197, 0.3199999928474426, 0.30000001192092896, 0.30000001192092896, 0.30000001192092896, 0.23999999463558197, 0.30000001192092896, 0.30000001192092896, 0.30000001192092896, 0.23999999463558197, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3400000035762787, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.25999999046325684, 0.2800000011920929, 0.25999999046325684, 0.2800000011920929, 0.25999999046325684, 0.3400000035762787, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.25999999046325684, 0.23999999463558197, 0.25999999046325684, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.25999999046325684, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.25999999046325684, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.23999999463558197, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.25999999046325684, 0.23999999463558197, 0.25999999046325684, 0.23999999463558197, 0.23999999463558197, 0.25999999046325684, 0.2800000011920929, 0.25999999046325684, 0.25999999046325684, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.2800000011920929, 0.3199999928474426, 0.2800000011920929, 0.2800000011920929, 0.3199999928474426, 0.3199999928474426, 0.2800000011920929, 0.2800000011920929, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.2800000011920929, 0.3199999928474426, 0.2800000011920929, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3400000035762787, 0.3199999928474426, 0.3400000035762787, 0.3400000035762787, 0.3199999928474426, 0.3400000035762787, 0.3400000035762787, 0.3400000035762787, 0.3199999928474426, 0.3400000035762787, 0.3400000035762787, 0.3199999928474426, 0.3400000035762787, 0.3199999928474426, 0.36000001430511475, 0.3400000035762787, 0.36000001430511475, 0.3400000035762787, 0.36000001430511475, 0.3199999928474426, 0.3400000035762787, 0.3400000035762787, 0.3199999928474426, 0.3400000035762787, 0.36000001430511475, 0.3199999928474426, 0.3400000035762787, 0.36000001430511475, 0.3400000035762787, 0.3400000035762787, 0.3400000035762787, 0.36000001430511475, 0.3400000035762787, 0.3400000035762787, 0.3400000035762787, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3799999952316284, 0.3799999952316284, 0.36000001430511475, 0.3799999952316284, 0.3400000035762787, 0.3400000035762787, 0.3799999952316284, 0.36000001430511475, 0.36000001430511475, 0.3400000035762787, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.36000001430511475, 0.4399999976158142, 0.36000001430511475, 0.47999998927116394, 0.36000001430511475, 0.4399999976158142, 0.4000000059604645, 0.47999998927116394, 0.4000000059604645, 0.4000000059604645, 0.36000001430511475, 0.4399999976158142, 0.4399999976158142, 0.4000000059604645, 0.36000001430511475, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.36000001430511475, 0.4000000059604645, 0.4399999976158142, 0.4399999976158142, 0.4000000059604645, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.36000001430511475, 0.36000001430511475, 0.4399999976158142, 0.4000000059604645, 0.4399999976158142, 0.4000000059604645, 0.4000000059604645, 0.4399999976158142, 0.47999998927116394, 0.4399999976158142, 0.47999998927116394, 0.4399999976158142, 0.4000000059604645, 0.47999998927116394, 0.36000001430511475, 0.4000000059604645, 0.47999998927116394, 0.47999998927116394, 0.4000000059604645, 0.4000000059604645, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.4399999976158142, 0.47999998927116394, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.47999998927116394, 0.47999998927116394, 0.4399999976158142, 0.4399999976158142, 0.4399999976158142, 0.47999998927116394, 0.47999998927116394, 0.47999998927116394, 0.5, 0.47999998927116394, 0.3799999952316284, 0.30000001192092896, 0.46000000834465027, 0.30000001192092896, 0.30000001192092896, 0.3400000035762787, 0.3400000035762787, 0.3400000035762787, 0.3199999928474426, 0.3199999928474426, 0.36000001430511475, 0.3199999928474426, 0.3400000035762787, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.3199999928474426, 0.30000001192092896, 0.30000001192092896, 0.30000001192092896, 0.36000001430511475, 0.3199999928474426, 0.3799999952316284, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.4000000059604645, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.4000000059604645, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.41999998688697815, 0.5, 0.5, 0.41999998688697815, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.41999998688697815, 0.5199999809265137, 0.5199999809265137, 0.5199999809265137, 0.4399999976158142, 0.5199999809265137, 0.5199999809265137, 0.5199999809265137, 0.5, 0.5199999809265137, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5199999809265137, 0.5400000214576721, 0.5199999809265137, 0.5199999809265137, 0.5199999809265137, 0.5400000214576721, 0.5199999809265137, 0.5400000214576721, 0.5199999809265137, 0.5600000023841858, 0.5199999809265137, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5199999809265137, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5600000023841858, 0.5400000214576721, 0.5600000023841858, 0.5600000023841858, 0.5400000214576721, 0.5600000023841858, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5600000023841858, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5400000214576721, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5400000214576721, 0.5600000023841858, 0.5600000023841858, 0.5400000214576721, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5799999833106995, 0.5799999833106995, 0.5600000023841858, 0.5799999833106995, 0.5799999833106995, 0.5799999833106995, 0.5600000023841858, 0.5600000023841858, 0.5799999833106995, 0.5600000023841858, 0.5600000023841858, 0.5600000023841858, 0.5799999833106995, 0.5799999833106995, 0.5799999833106995, 0.5799999833106995]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['loss'])\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = [0.0096, 0.0013, 0.0014, 0.00096996, 0.00095061, 0.00076971, 0.0002997, 0.0006391, 0.0005801, 0.00048286]\n",
    "validation_loss = [0.0012, 0.0011, 0.00074558, 0.00071942, 0.00068666, 0.00050246, 0.00049399, 0.00049060, 0.0004091, 0.00040874]\n",
    "validation_accuracy = [0.2627, 0.3425, 0.2618, 0.4119, 0.4112, 0.6048, 0.8779, 0.7888, 0.7932, 0.8410]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Here, we evaluate the neural network with the test data.\n",
    "\n",
    "This block stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 11s 2ms/step - loss: 0.0527 - accuracy: 0.4944 - binary_accuracy: 0.9400\n",
      "Test loss: 0.05265556275844574\n",
      "Test accuracy: 0.49443262815475464\n"
     ]
    }
   ],
   "source": [
    "results = neural_network.evaluate(test_samples, test_labels, batch_size=batch_size)\n",
    "print(\"Test loss: {}\".format(results[0]))\n",
    "print(\"Test accuracy: {}\".format(results[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plaintext Recovery in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = [predict_sample(neural_network, test_samples[i]) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [get_metrics((predictions[i], test_labels[i])) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct bytes: 1487\n",
      "Byte accuracy: 0.7435\n",
      "Correct predictions: 681\n",
      "Prediction accuracy: 0.681\n"
     ]
    }
   ],
   "source": [
    "correct_bytes = 0\n",
    "correct_predictions = 0\n",
    "for m in metrics:\n",
    "    correct_bytes += m[0]\n",
    "    correct_predictions += m[1]\n",
    "                             \n",
    "print(\"Correct bytes: {}\".format(correct_bytes))\n",
    "print(\"Byte accuracy: {}\".format(correct_bytes/(2*1000)))\n",
    "print(\"Correct predictions: {}\".format(correct_predictions))\n",
    "print(\"Prediction accuracy: {}\".format(correct_predictions/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: do || dm\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: m  || m \n",
      "prediction: no || no\n",
      "prediction: t. || t.\n",
      "prediction: \n",
      "\n",
      " || \n",
      "\n",
      "\n",
      "prediction: 33 || ne\n",
      "prediction: :3 || re\n",
      "prediction: 3  || 3 \n",
      "prediction: An || An\n",
      "prediction: d  || d \n",
      "prediction: wh || wh\n",
      "prediction: en || en\n",
      "prediction:  t ||  t\n",
      "prediction: hi || hi\n",
      "prediction: s  || s \n",
      "prediction: co || ai\n",
      "prediction: me || me\n",
      "prediction: th || th\n",
      "prediction:  t ||  t\n",
      "prediction: o  || n)\n",
      "prediction: pa ||  e\n",
      "prediction: ss || ss\n",
      "prediction: ,  || , \n",
      "prediction: (l || \n",
      "a\n",
      "prediction: o, || nl\n",
      "prediction:  i ||  i\n",
      "prediction: t  || t \n",
      "prediction: wi || wi\n",
      "prediction: ll || ll\n",
      "prediction:  c ||  c\n",
      "prediction: om || om\n",
      "prediction: e, || cl\n",
      "prediction: )  || 8(\n",
      "prediction: th || th\n",
      "prediction: en || en\n",
      "prediction:  s ||  s\n",
      "prediction: ha || ha\n",
      "prediction: ll || ll\n",
      "prediction: \n",
      "t || Ce\n",
      "prediction: he || he\n",
      "prediction: y  || x`\n",
      "prediction: kn || kn\n",
      "prediction: ow || Og\n",
      "prediction:  t ||  t\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: a  || a \n",
      "prediction: pr ||  w\n",
      "prediction: op || l \n",
      "prediction: he || he\n",
      "prediction: t  || t \n",
      "prediction: ha || ha\n",
      "prediction: th || th\n",
      "prediction:  b ||  b\n",
      "prediction: ee || ee\n",
      "prediction: n  || n \n",
      "prediction: am || am\n",
      "prediction: on || on\n",
      "prediction: g  || g \n",
      "prediction: th || th\n",
      "prediction: em || um\n",
      "prediction: .\n",
      " || .\n",
      "\n",
      "prediction: \n",
      "3 || he\n",
      "prediction: 4: || vh\n",
      "prediction: 1  || 1 \n",
      "prediction: An || An\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: wo || Gn\n",
      "prediction: rd || ri\n",
      "prediction:  o ||  o\n",
      "prediction: f  || f \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: LO || Go\n",
      "prediction: RD || f!\n",
      "prediction:  c ||  c\n",
      "prediction: am || am\n",
      "prediction: e  || e \n",
      "prediction: un || un\n",
      "prediction: to || do\n",
      "prediction:  m ||  m\n",
      "prediction: e, || cl\n",
      "prediction:  s ||  s\n",
      "prediction: ay || ai\n",
      "prediction: in || in\n",
      "prediction: g, || gn\n",
      "prediction:  3 || je\n",
      "prediction: 4: || vh\n",
      "prediction: 2  || 2 \n",
      "prediction: So || ch\n",
      "prediction: n  || n \n",
      "prediction: of || of\n",
      "prediction:  m ||  m\n",
      "prediction: an || an\n",
      "prediction: ,\n",
      " || sm\n",
      "prediction: pr ||  w\n",
      "prediction: op || l \n",
      "prediction: he || he\n",
      "prediction: sy || \n",
      "prediction:  a ||  a\n",
      "prediction: ga || ga\n",
      "prediction: in || in\n",
      "prediction: st || st\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: ds || `s\n",
      "prediction:  o ||  o\n",
      "prediction: f  || f \n",
      "prediction: Is || ip\n",
      "prediction: ra || re\n",
      "prediction: el || ql\n",
      "prediction: ,  || , \n",
      "prediction: pr ||  w\n",
      "prediction: op || l \n",
      "prediction: he || he\n",
      "prediction: sy || \n",
      "prediction: ,  || , \n",
      "prediction: an || an\n",
      "prediction: d  || d \n",
      "prediction: sa || sa\n",
      "prediction: y  || x`\n",
      "prediction: un || un\n",
      "prediction: to || do\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: m, || al\n",
      "prediction: \n",
      "T || \n",
      "T\n",
      "prediction: hu || je\n",
      "prediction: s  || s \n",
      "prediction: sa || sa\n",
      "prediction: it || it\n",
      "prediction: h  || h \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: Lo || o.\n",
      "prediction: rd || ri\n",
      "prediction:  G ||  G\n",
      "prediction: OD || e`\n",
      "prediction:  u ||  u\n",
      "prediction: nt || nt\n",
      "prediction: o  || n)\n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: sh || qh\n",
      "prediction: ep || ep\n",
      "prediction: he || he\n",
      "prediction: rd || ri\n",
      "prediction: s; || oc\n",
      "prediction:  W || \"C\n",
      "prediction: oe || oe\n",
      "prediction:  b ||  b\n",
      "prediction: e  || e \n",
      "prediction: to || do\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: ds || `s\n",
      "prediction:  o ||  o\n",
      "prediction: f\n",
      " || \n",
      "prediction: Is || ip\n",
      "prediction: ra || re\n",
      "prediction: el || ql\n",
      "prediction:  t ||  t\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: do || dm\n",
      "prediction:  f ||  f\n",
      "prediction: ee || ee\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: em || um\n",
      "prediction: se || sa\n",
      "prediction: lv || :a\n",
      "prediction: es || es\n",
      "prediction: !  || )0\n",
      "prediction: sh || qh\n",
      "prediction: ou || ou\n",
      "prediction: ld || md\n",
      "prediction:  n || `m\n",
      "prediction: ot || mt\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: ds || `s\n",
      "prediction:  f ||  f\n",
      "prediction: ee || ee\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: e\n",
      " || a\n",
      "\n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: ks || oc\n",
      "prediction: ?  || \n",
      "\u0001\n",
      "prediction:  3 || je\n",
      "prediction: 4: || vh\n",
      "prediction: 3  || 3 \n",
      "prediction: Ye || qh\n",
      "prediction:  e ||  e\n",
      "prediction: at || at\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  f ||  f\n",
      "prediction: at || at\n",
      "prediction: ,  || , \n",
      "prediction: an || an\n",
      "prediction: d  || d \n",
      "prediction: ye || qe\n",
      "prediction:  c ||  c\n",
      "prediction: lo || oo\n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: yo || in\n",
      "prediction: u  || s`\n",
      "prediction: wi || wi\n",
      "prediction: th || th\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  w ||  w\n",
      "prediction: oo || oo\n",
      "prediction: l, || al\n",
      "prediction:  y || 0`\n",
      "prediction: e  || e \n",
      "prediction: ki || mo\n",
      "prediction: ll || ll\n",
      "prediction: \n",
      "t || Ce\n",
      "prediction: he || he\n",
      "prediction: m  || m \n",
      "prediction: th || th\n",
      "prediction: at || at\n",
      "prediction:  a ||  a\n",
      "prediction: re || re\n",
      "prediction:  f ||  f\n",
      "prediction: ed || ed\n",
      "prediction: :  || : \n",
      "prediction: bu || cu\n",
      "prediction: t  || t \n",
      "prediction: ye || qe\n",
      "prediction:  f ||  f\n",
      "prediction: ee || ee\n",
      "prediction: d  || d \n",
      "prediction: no || no\n",
      "prediction: t  || t \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: k. || kn\n",
      "prediction: \n",
      "\n",
      " || \n",
      "\n",
      "\n",
      "prediction: 34 || na\n",
      "prediction: :4 || ze\n",
      "prediction:  T || rg\n",
      "prediction: he || he\n",
      "prediction:  d ||  d\n",
      "prediction: is || is\n",
      "prediction: ea || ea\n",
      "prediction: se || sa\n",
      "prediction: d  || d \n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction:  y || 0`\n",
      "prediction: e  || e \n",
      "prediction: no || no\n",
      "prediction: t  || t \n",
      "prediction: st || st\n",
      "prediction: re || re\n",
      "prediction: ng || ng\n",
      "prediction: th || th\n",
      "prediction: en || en\n",
      "prediction: ed || ed\n",
      "prediction: ,  || , \n",
      "prediction: ne || ne\n",
      "prediction: it || it\n",
      "prediction: he || he\n",
      "prediction: r  || r \n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction:  y || 0`\n",
      "prediction: e  || e \n",
      "prediction: he || he\n",
      "prediction: al || al\n",
      "prediction: ed || ed\n",
      "prediction: \n",
      "t || Ce\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: wh || wh\n",
      "prediction: ic || ic\n",
      "prediction: h  || h \n",
      "prediction: wa || wa\n",
      "prediction: s  || s \n",
      "prediction: si || si\n",
      "prediction: ck || a \n",
      "prediction: ,  || , \n",
      "prediction: ne || ne\n",
      "prediction: it || it\n",
      "prediction: he || he\n",
      "prediction: r  || r \n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction:  y || 0`\n",
      "prediction: e  || e \n",
      "prediction: bo || bo\n",
      "prediction: un || un\n",
      "prediction: d  || d \n",
      "prediction: up || up\n",
      "prediction:  t ||  t\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: wh || wh\n",
      "prediction: ic || ic\n",
      "prediction: h  || h \n",
      "prediction: wa || wa\n",
      "prediction: s  || s \n",
      "prediction: br || sr\n",
      "prediction: ok || o`\n",
      "prediction: en || en\n",
      "prediction: ,\n",
      " || sm\n",
      "prediction: ne || ne\n",
      "prediction: it || it\n",
      "prediction: he || he\n",
      "prediction: r  || r \n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction:  y || 0`\n",
      "prediction: e  || e \n",
      "prediction: br || sr\n",
      "prediction: ou || ou\n",
      "prediction: gh || gh\n",
      "prediction: t  || t \n",
      "prediction: ag || aw\n",
      "prediction: ai || an\n",
      "prediction: n  || n \n",
      "prediction: th || th\n",
      "prediction: at || at\n",
      "prediction:  w ||  w\n",
      "prediction: hi || hi\n",
      "prediction: ch || ch\n",
      "prediction:  w ||  w\n",
      "prediction: as || as\n",
      "prediction:  d ||  d\n",
      "prediction: ri || vi\n",
      "prediction: ve || ve\n",
      "prediction: n  || n \n",
      "prediction: aw || cg\n",
      "prediction: ay || ai\n",
      "prediction: ,  || , \n",
      "prediction: ne || ne\n",
      "prediction: it || it\n",
      "prediction: he || he\n",
      "prediction: r  || r \n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction: \n",
      "y || n \n",
      "prediction: e  || e \n",
      "prediction: so || aj\n",
      "prediction: ug || cg\n",
      "prediction: ht || ht\n",
      "prediction:  t ||  t\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: wh || wh\n",
      "prediction: ic || ic\n",
      "prediction: h  || h \n",
      "prediction: wa || wa\n",
      "prediction: s  || s \n",
      "prediction: lo || oo\n",
      "prediction: st || st\n",
      "prediction: ;  || ; \n",
      "prediction: bu || cu\n",
      "prediction: t  || t \n",
      "prediction: wi || wi\n",
      "prediction: th || th\n",
      "prediction:  f ||  f\n",
      "prediction: or || or\n",
      "prediction: ce || ce\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction:  w ||  w\n",
      "prediction: it || it\n",
      "prediction: h  || h \n",
      "prediction: cr || uj\n",
      "prediction: ue || we\n",
      "prediction: lt || ld\n",
      "prediction: y  || x`\n",
      "prediction: ha || ha\n",
      "prediction: ve || ve\n",
      "prediction:  y || 0`\n",
      "prediction: e\n",
      " || a\n",
      "\n",
      "prediction: ru || qd\n",
      "prediction: le || le\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: em || um\n",
      "prediction: .\n",
      " || .\n",
      "\n",
      "prediction: \n",
      "3 || he\n",
      "prediction: 4: || vh\n",
      "prediction: 5  || s \n",
      "prediction: An || An\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: ey || di\n",
      "prediction:  w ||  w\n",
      "prediction: er || er\n",
      "prediction: e  || e \n",
      "prediction: sc || ci\n",
      "prediction: at || at\n",
      "prediction: te || te\n",
      "prediction: re || re\n",
      "prediction: d, || p,\n",
      "prediction:  b ||  b\n",
      "prediction: ec || ek\n",
      "prediction: au || ce\n",
      "prediction: se || sa\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: re || re\n",
      "prediction:  i ||  i\n",
      "prediction: s  || s \n",
      "prediction: no || no\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: d: || f*\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: y\n",
      " || (,\n",
      "prediction: be || be\n",
      "prediction: ca || ca\n",
      "prediction: me || me\n",
      "prediction:  m ||  m\n",
      "prediction: ea || ea\n",
      "prediction: t  || t \n",
      "prediction: to || do\n",
      "prediction:  a ||  a\n",
      "prediction: ll || ll\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  b ||  b\n",
      "prediction: ea || ea\n",
      "prediction: st || st\n",
      "prediction: s  || s \n",
      "prediction: of || of\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  f ||  f\n",
      "prediction: ie || ie\n",
      "prediction: ld || md\n",
      "prediction: ,  || , \n",
      "prediction: wh || wh\n",
      "prediction: en || en\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: y  || x`\n",
      "prediction: we || we\n",
      "prediction: re || re\n",
      "prediction:  s ||  s\n",
      "prediction: ca || ca\n",
      "prediction: tt ||  l\n",
      "prediction: er || er\n",
      "prediction: ed || ed\n",
      "prediction: .\n",
      " || .\n",
      "\n",
      "prediction: \n",
      "3 || he\n",
      "prediction: 4: || vh\n",
      "prediction: 6  || 6 \n",
      "prediction: My || Jh\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ep || ep\n",
      "prediction:  w ||  w\n",
      "prediction: an || an\n",
      "prediction: de || de\n",
      "prediction: re || re\n",
      "prediction: d  || d \n",
      "prediction: th || th\n",
      "prediction: ro || ro\n",
      "prediction: ug || cg\n",
      "prediction: h  || h \n",
      "prediction: al || al\n",
      "prediction: l  || m \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: mo || mo\n",
      "prediction: un || un\n",
      "prediction: ta || te\n",
      "prediction: in || in\n",
      "prediction: s, || s,\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction:  u ||  u\n",
      "prediction: po || `o\n",
      "prediction: n  || n \n",
      "prediction: ev || up\n",
      "prediction: er || er\n",
      "prediction: y  || x`\n",
      "prediction: hi || hi\n",
      "prediction: gh || gh\n",
      "prediction: \n",
      "h || hi\n",
      "prediction: il || il\n",
      "prediction: l: || Jk\n",
      "prediction:  y || 0`\n",
      "prediction: ea || ea\n",
      "prediction: ,  || , \n",
      "prediction: my || Hi\n",
      "prediction:  f ||  f\n",
      "prediction: lo || oo\n",
      "prediction: ck || a \n",
      "prediction:  w ||  w\n",
      "prediction: as || as\n",
      "prediction:  s ||  s\n",
      "prediction: ca || ca\n",
      "prediction: tt ||  l\n",
      "prediction: er || er\n",
      "prediction: ed || ed\n",
      "prediction:  u ||  u\n",
      "prediction: po || `o\n",
      "prediction: n  || n \n",
      "prediction: al || al\n",
      "prediction: l  || m \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: fa || fa\n",
      "prediction: ce || ce\n",
      "prediction:  o ||  o\n",
      "prediction: f  || f \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: ea || ea\n",
      "prediction: rt || rt\n",
      "prediction: h, || (l\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction: \n",
      "n || hd\n",
      "prediction: on || on\n",
      "prediction: e  || e \n",
      "prediction: di || di\n",
      "prediction: d  || d \n",
      "prediction: se || sa\n",
      "prediction: ar || ar\n",
      "prediction: ch || ch\n",
      "prediction:  o ||  o\n",
      "prediction: r  || r \n",
      "prediction: se || sa\n",
      "prediction: ek || u \n",
      "prediction:  a ||  a\n",
      "prediction: ft || id\n",
      "prediction: er || er\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: m. || an\n",
      "prediction: \n",
      "\n",
      " || \n",
      "\n",
      "\n",
      "prediction: 34 || na\n",
      "prediction: :7 || z.\n",
      "prediction:  T || rg\n",
      "prediction: he || he\n",
      "prediction: re || re\n",
      "prediction: fo || fo\n",
      "prediction: re || re\n",
      "prediction: ,  || , \n",
      "prediction: ye || qe\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: ds || `s\n",
      "prediction: ,  || , \n",
      "prediction: he || he\n",
      "prediction: ar || ar\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  w ||  w\n",
      "prediction: or || or\n",
      "prediction: d  || d \n",
      "prediction: of || of\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  L || 0c\n",
      "prediction: OR || qh\n",
      "prediction: D; || nn\n",
      "prediction:  3 || je\n",
      "prediction: 4: || vh\n",
      "prediction: 8  || 8 \n",
      "prediction: As || ap\n",
      "prediction:  I || 0h\n",
      "prediction: \n",
      "l || h.\n",
      "prediction: iv || iv\n",
      "prediction: e, || cl\n",
      "prediction:  s ||  s\n",
      "prediction: ai || an\n",
      "prediction: th || th\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  L || 0c\n",
      "prediction: or || or\n",
      "prediction: d  || d \n",
      "prediction: GO || j.\n",
      "prediction: D, || r \n",
      "prediction:  s ||  s\n",
      "prediction: ur || dg\n",
      "prediction: el || ql\n",
      "prediction: y  || x`\n",
      "prediction: be || be\n",
      "prediction: ca || ca\n",
      "prediction: us || \"C\n",
      "prediction: e  || e \n",
      "prediction: my || Hi\n",
      "prediction:  f ||  f\n",
      "prediction: lo || oo\n",
      "prediction: ck || a \n",
      "prediction:  b ||  b\n",
      "prediction: ec || ek\n",
      "prediction: am || am\n",
      "prediction: e  || e \n",
      "prediction: a  || a \n",
      "prediction: pr ||  w\n",
      "prediction: ey || di\n",
      "prediction: ,  || , \n",
      "prediction: an || an\n",
      "prediction: d\n",
      " || Jo\n",
      "prediction: my || Hi\n",
      "prediction:  f ||  f\n",
      "prediction: lo || oo\n",
      "prediction: ck || a \n",
      "prediction:  b ||  b\n",
      "prediction: ec || ek\n",
      "prediction: am || am\n",
      "prediction: e  || e \n",
      "prediction: me || me\n",
      "prediction: at || at\n",
      "prediction:  t ||  t\n",
      "prediction: o  || n)\n",
      "prediction: ev || up\n",
      "prediction: er || er\n",
      "prediction: y  || x`\n",
      "prediction: be || be\n",
      "prediction: as || as\n",
      "prediction: t  || t \n",
      "prediction: of || of\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  f ||  f\n",
      "prediction: ie || ie\n",
      "prediction: ld || md\n",
      "prediction: ,  || , \n",
      "prediction: be || be\n",
      "prediction: ca || ca\n",
      "prediction: us || \"C\n",
      "prediction: e  || e \n",
      "prediction: th || th\n",
      "prediction: er || er\n",
      "prediction: e  || e \n",
      "prediction: wa || wa\n",
      "prediction: s  || s \n",
      "prediction: no || no\n",
      "prediction: \n",
      "s || ge\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: d, || p,\n",
      "prediction:  n || `m\n",
      "prediction: ei || en\n",
      "prediction: th || th\n",
      "prediction: er || er\n",
      "prediction:  d ||  d\n",
      "prediction: id || id\n",
      "prediction:  m ||  m\n",
      "prediction: y  || x`\n",
      "prediction: sh || qh\n",
      "prediction: ep || ep\n",
      "prediction: he || he\n",
      "prediction: rd || ri\n",
      "prediction: s  || s \n",
      "prediction: se || sa\n",
      "prediction: ar || ar\n",
      "prediction: ch || ch\n",
      "prediction:  f ||  f\n",
      "prediction: or || or\n",
      "prediction:  m ||  m\n",
      "prediction: y  || x`\n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: k, || i*\n",
      "prediction:  b ||  b\n",
      "prediction: ut || sx\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: \n",
      "s || ge\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: ds || `s\n",
      "prediction:  f ||  f\n",
      "prediction: ed || ed\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: ms || ks\n",
      "prediction: el || ql\n",
      "prediction: ve || ve\n",
      "prediction: s, || s,\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction:  f ||  f\n",
      "prediction: ed || ed\n",
      "prediction:  n || `m\n",
      "prediction: ot || mt\n",
      "prediction:  m ||  m\n",
      "prediction: y  || x`\n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: k; || s \n",
      "prediction:  3 || je\n",
      "prediction: 4: || vh\n",
      "prediction: 9  || 8 \n",
      "prediction: Th || th\n",
      "prediction: er || er\n",
      "prediction: ef || ua\n",
      "prediction: or || or\n",
      "prediction: e, || cl\n",
      "prediction:  O ||  C\n",
      "prediction:  y || 0`\n",
      "prediction: e\n",
      " || a\n",
      "\n",
      "prediction: sh || qh\n",
      "prediction: ep || ep\n",
      "prediction: he || he\n",
      "prediction: rd || ri\n",
      "prediction: s, || s,\n",
      "prediction:  h ||  h\n",
      "prediction: ea || ea\n",
      "prediction: r  || r \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: wo || Gn\n",
      "prediction: rd || ri\n",
      "prediction:  o ||  o\n",
      "prediction: f  || f \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: LO || Go\n",
      "prediction: RD || f!\n",
      "prediction: ;  || ; \n",
      "prediction: 34 || na\n",
      "prediction: :1 || r6\n",
      "prediction: 0  || : \n",
      "prediction: Th || th\n",
      "prediction: us || \"C\n",
      "prediction:  s ||  s\n",
      "prediction: ai || an\n",
      "prediction: th || th\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  L || 0c\n",
      "prediction: or || or\n",
      "prediction: d  || d \n",
      "prediction: GO || j.\n",
      "prediction: D; || nn\n",
      "prediction: \n",
      "B || o`\n",
      "prediction: eh || q`\n",
      "prediction: ol || ol\n",
      "prediction: d, || p,\n",
      "prediction:  I || 0h\n",
      "prediction:  a ||  a\n",
      "prediction: m  || m \n",
      "prediction: ag || aw\n",
      "prediction: ai || an\n",
      "prediction: ns || fg\n",
      "prediction: t  || t \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: sh || qh\n",
      "prediction: ep || ep\n",
      "prediction: he || he\n",
      "prediction: rd || ri\n",
      "prediction: s; || oc\n",
      "prediction:  a ||  a\n",
      "prediction: nd || nd\n",
      "prediction:  I || 0h\n",
      "prediction:  w ||  w\n",
      "prediction: il || il\n",
      "prediction: l  || m \n",
      "prediction: re || re\n",
      "prediction: qu || sa\n",
      "prediction: ir || ir\n",
      "prediction: e  || e \n",
      "prediction: my || Hi\n",
      "prediction:  f ||  f\n",
      "prediction: lo || oo\n",
      "prediction: ck || a \n",
      "prediction:  a ||  a\n",
      "prediction: t\n",
      " || n$\n",
      "prediction: th || th\n",
      "prediction: ei || en\n",
      "prediction: r  || r \n",
      "prediction: ha || ha\n",
      "prediction: nd || nd\n",
      "prediction: ,  || , \n",
      "prediction: an || an\n",
      "prediction: d  || d \n",
      "prediction: ca || ca\n",
      "prediction: us || \"C\n",
      "prediction: e  || e \n",
      "prediction: th || th\n",
      "prediction: em || um\n",
      "prediction:  t ||  t\n",
      "prediction: o  || n)\n",
      "prediction: ce || ce\n",
      "prediction: as || as\n",
      "prediction: e  || e \n",
      "prediction: fr || gr\n",
      "prediction: om || om\n",
      "prediction:  f ||  f\n",
      "prediction: ee || ee\n",
      "prediction: di || di\n",
      "prediction: ng || ng\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  f ||  f\n",
      "prediction: lo || oo\n",
      "prediction: ck || a \n",
      "prediction: ;  || ; \n",
      "prediction: ne || ne\n",
      "prediction: it || it\n",
      "prediction: he || he\n",
      "prediction: r\n",
      " || ~(\n",
      "prediction: sh || qh\n",
      "prediction: al || al\n",
      "prediction: l  || m \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: sh || qh\n",
      "prediction: ep || ep\n",
      "prediction: he || he\n",
      "prediction: rd || ri\n",
      "prediction: s  || s \n",
      "prediction: fe || fe\n",
      "prediction: ed || ed\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: ms || ks\n",
      "prediction: el || ql\n",
      "prediction: ve || ve\n",
      "prediction: s  || s \n",
      "prediction: an || an\n",
      "prediction: y  || x`\n",
      "prediction: mo || mo\n",
      "prediction: re || re\n",
      "prediction: ;  || ; \n",
      "prediction: fo || fo\n",
      "prediction: r  || r \n",
      "prediction: I  || (g\n",
      "prediction: wi || wi\n",
      "prediction: ll || ll\n",
      "prediction:  d ||  d\n",
      "prediction: el || ql\n",
      "prediction: iv || iv\n",
      "prediction: er || er\n",
      "prediction:  m ||  m\n",
      "prediction: y\n",
      " || (,\n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: k  || k \n",
      "prediction: fr || gr\n",
      "prediction: om || om\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: ir || ir\n",
      "prediction:  m ||  m\n",
      "prediction: ou || ou\n",
      "prediction: th || th\n",
      "prediction: ,  || , \n",
      "prediction: th || th\n",
      "prediction: at || at\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: y  || x`\n",
      "prediction: ma || ma\n",
      "prediction: y  || x`\n",
      "prediction: no || no\n",
      "prediction: t  || t \n",
      "prediction: be || be\n",
      "prediction:  m ||  m\n",
      "prediction: ea || ea\n",
      "prediction: t  || t \n",
      "prediction: fo || fo\n",
      "prediction: r  || r \n",
      "prediction: th || th\n",
      "prediction: em || um\n",
      "prediction: .\n",
      " || .\n",
      "\n",
      "prediction: \n",
      "3 || he\n",
      "prediction: 4: || vh\n",
      "prediction: 11 || sr\n",
      "prediction:  F ||  F\n",
      "prediction: or || or\n",
      "prediction:  t ||  t\n",
      "prediction: hu || je\n",
      "prediction: s  || s \n",
      "prediction: sa || sa\n",
      "prediction: it || it\n",
      "prediction: h  || h \n",
      "prediction: th || th\n",
      "prediction: e  || e \n",
      "prediction: Lo || o.\n",
      "prediction: rd || ri\n",
      "prediction:  G ||  G\n",
      "prediction: OD || e`\n",
      "prediction: ;  || ; \n",
      "prediction: Be || be\n",
      "prediction: ho || je\n",
      "prediction: ld || md\n",
      "prediction: ,  || , \n",
      "prediction: I, || \n",
      "h\n",
      "prediction:  e ||  e\n",
      "prediction: ve || ve\n",
      "prediction: n  || n \n",
      "prediction: I, || \n",
      "h\n",
      "prediction:  w ||  w\n",
      "prediction: il || il\n",
      "prediction: l  || m \n",
      "prediction: bo || bo\n",
      "prediction: th || th\n",
      "prediction:  s ||  s\n",
      "prediction: ea || ea\n",
      "prediction: rc || ra\n",
      "prediction: h\n",
      " || \n",
      "\n",
      "\n",
      "prediction: my || Hi\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ep || ep\n",
      "prediction: ,  || , \n",
      "prediction: an || an\n",
      "prediction: d  || d \n",
      "prediction: se || sa\n",
      "prediction: ek || u \n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction: m  || m \n",
      "prediction: ou || ou\n",
      "prediction: t. || t.\n",
      "prediction: \n",
      "\n",
      " || \n",
      "\n",
      "\n",
      "prediction: 34 || na\n",
      "prediction: :1 || r6\n",
      "prediction: 2  || 2 \n",
      "prediction: As || ap\n",
      "prediction:  a ||  a\n",
      "prediction:  s ||  s\n",
      "prediction: he || he\n",
      "prediction: ph ||  i\n",
      "prediction: er || er\n",
      "prediction: d  || d \n",
      "prediction: se || sa\n",
      "prediction: ek || u \n",
      "prediction: et || et\n",
      "prediction: h  || h \n",
      "prediction: ou || ou\n",
      "prediction: t  || t \n",
      "prediction: hi || hi\n",
      "prediction: s  || s \n",
      "prediction: fl || vl\n",
      "prediction: oc || oc\n",
      "prediction: k  || k \n",
      "prediction: in || in\n",
      "prediction:  t ||  t\n",
      "prediction: he || he\n",
      "prediction:  d ||  d\n",
      "prediction: ay || ai\n",
      "prediction:  t ||  t\n",
      "prediction: ha || ha\n",
      "prediction: t  || t \n",
      "prediction: he || he\n",
      "prediction:  i ||  i\n",
      "prediction: s  || s \n",
      "prediction: am || am\n",
      "prediction: on || on\n",
      "prediction: g\n",
      " || r*\n",
      "prediction: hi || hi\n",
      "prediction: s  || s \n",
      "prediction: sh || qh\n",
      "prediction: ee || ee\n",
      "prediction: p  || r \n",
      "prediction: th || th\n",
      "prediction: at || at\n",
      "prediction:  a ||  a\n",
      "prediction: re || re\n",
      "prediction:  s ||  s\n",
      "prediction: ca || ca\n",
      "prediction: tt ||  l\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print(\"prediction: \" + prediction_to_string(test_labels[i]) + \" || \" + prediction_to_string(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(neural_network, \"s_aes_feed_forward_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
