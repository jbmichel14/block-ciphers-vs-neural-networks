{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPECK32/64 Key Recovery: 1st bit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this attack, we aim to recover the first bit of the key using a binary classifier. We train a neural network on plaintext-ciphertext pairs as samples and the first bit of the key as labels. We use SPECK32/64 with the smallest configuration (64-bit key and 32-bit blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 18:37:41.746833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from dataset.datasets import SPECKDatasetCiphertextPlaintextPairKey\n",
    "from pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: dataset/assets/moby-dick.txt\n",
      "Error: dataset/assets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "data = SPECKDatasetCiphertextPlaintextPairKey(64, 32, 'large')\n",
    "\n",
    "train_labels, train_samples, test_labels, test_samples = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Labels Shape: (5782806,)\n",
      "===== Label Shape: (64,)\n",
      "===== Training Samples Shape: (5782806, 160)\n",
      "===== Sample Shape: (160,)\n",
      "===== Testing Labels Shape: (2478345,)\n",
      "===== Testing Samples Shape: (2478345, 160)\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(train_labels, train_samples, test_labels, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We add 1 preprocessing step:\n",
    "As our goal is only to recover the first bit of the key, we only keep the first bit of the labels to train our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.astype(float)\n",
    "test_samples = test_samples.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_1st_bit = np.array([l[0] for l in train_labels]).astype(np.float32)\n",
    "test_labels_1st_bit = np.array([l[0] for l in test_labels]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Labels Shape: (5782806,)\n",
      "===== Label Shape: ()\n",
      "===== Training Samples Shape: (5782806, 160)\n",
      "===== Sample Shape: (160,)\n",
      "===== Testing Labels Shape: (2478345,)\n",
      "===== Testing Samples Shape: (2478345, 160)\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(train_labels_1st_bit, train_samples, test_labels_1st_bit, test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.shape(train_samples[0])\n",
    "\n",
    "# output dimension\n",
    "dim = 1\n",
    "\n",
    "# units per hidden layer\n",
    "units = 128\n",
    "\n",
    "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate=0.001,\n",
    "#     decay_steps=10000,\n",
    "#     decay_rate=-0.9)\n",
    "\n",
    "loss_scc = 'sparse_categorical_crossentropy'\n",
    "loss_mse = 'mse'\n",
    "loss_bce = 'binary_crossentropy'\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "metrics = ['binary_accuracy']\n",
    "epochs = 50\n",
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In this code block, we create the model, according to the parameters and the topology we want to achieve. \n",
    "We then compile it specifying the optimizer, the loss and the metrics we want outputted.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               20608     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,761\n",
      "Trainable params: 53,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Â Type of model\n",
    "neural_network = Sequential()\n",
    "\n",
    "# Input layer\n",
    "neural_network.add(Input(shape=input_shape))\n",
    "\n",
    "# Hidden layers\n",
    "# neural_network.add(BatchNormalization())\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "neural_network.add(Dense(units=dim, activation='sigmoid'))\n",
    "\n",
    "# Summary\n",
    "neural_network.summary()\n",
    "\n",
    "# Compile model\n",
    "neural_network.compile(optimizer=optimizer, loss=loss_bce, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this code block, we train the model. It outputs, for each epoch, the loss and metrics.\n",
    "\n",
    "This block mostly stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1157/1157 [==============================] - 19s 16ms/step - loss: 0.6933 - binary_accuracy: 0.5003\n",
      "Epoch 2/50\n",
      "1157/1157 [==============================] - 16s 13ms/step - loss: 0.6932 - binary_accuracy: 0.5011\n",
      "Epoch 3/50\n",
      "1157/1157 [==============================] - 16s 13ms/step - loss: 0.6931 - binary_accuracy: 0.5013\n",
      "Epoch 4/50\n",
      "1157/1157 [==============================] - 16s 13ms/step - loss: 0.6931 - binary_accuracy: 0.5015\n",
      "Epoch 5/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5017\n",
      "Epoch 6/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5015\n",
      "Epoch 7/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5019\n",
      "Epoch 8/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5019\n",
      "Epoch 9/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5021\n",
      "Epoch 10/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5023\n",
      "Epoch 11/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5022\n",
      "Epoch 12/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5025\n",
      "Epoch 13/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6931 - binary_accuracy: 0.5030\n",
      "Epoch 14/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6930 - binary_accuracy: 0.5034\n",
      "Epoch 15/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6930 - binary_accuracy: 0.5042\n",
      "Epoch 16/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6929 - binary_accuracy: 0.5044\n",
      "Epoch 17/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6928 - binary_accuracy: 0.5051\n",
      "Epoch 18/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6927 - binary_accuracy: 0.5059\n",
      "Epoch 19/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6926 - binary_accuracy: 0.5060\n",
      "Epoch 20/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6925 - binary_accuracy: 0.5070\n",
      "Epoch 21/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6923 - binary_accuracy: 0.5079\n",
      "Epoch 22/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6922 - binary_accuracy: 0.5082\n",
      "Epoch 23/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6920 - binary_accuracy: 0.5092\n",
      "Epoch 24/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6918 - binary_accuracy: 0.5094\n",
      "Epoch 25/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6916 - binary_accuracy: 0.5102\n",
      "Epoch 26/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6915 - binary_accuracy: 0.5105\n",
      "Epoch 27/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6913 - binary_accuracy: 0.5110\n",
      "Epoch 28/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6911 - binary_accuracy: 0.5115\n",
      "Epoch 29/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6910 - binary_accuracy: 0.5120\n",
      "Epoch 30/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6908 - binary_accuracy: 0.5122\n",
      "Epoch 31/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6906 - binary_accuracy: 0.5125\n",
      "Epoch 32/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6905 - binary_accuracy: 0.5129\n",
      "Epoch 33/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6903 - binary_accuracy: 0.5133\n",
      "Epoch 34/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6901 - binary_accuracy: 0.5135\n",
      "Epoch 35/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6900 - binary_accuracy: 0.5138\n",
      "Epoch 36/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6898 - binary_accuracy: 0.5138\n",
      "Epoch 37/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6897 - binary_accuracy: 0.5144\n",
      "Epoch 38/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6895 - binary_accuracy: 0.5145\n",
      "Epoch 39/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6894 - binary_accuracy: 0.5150\n",
      "Epoch 40/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6893 - binary_accuracy: 0.5153\n",
      "Epoch 41/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6891 - binary_accuracy: 0.5154\n",
      "Epoch 42/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6890 - binary_accuracy: 0.5155\n",
      "Epoch 43/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6889 - binary_accuracy: 0.5158\n",
      "Epoch 44/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6888 - binary_accuracy: 0.5159\n",
      "Epoch 45/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6886 - binary_accuracy: 0.5161\n",
      "Epoch 46/50\n",
      "1157/1157 [==============================] - 16s 14ms/step - loss: 0.6885 - binary_accuracy: 0.5165\n",
      "Epoch 47/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6884 - binary_accuracy: 0.5164\n",
      "Epoch 48/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6883 - binary_accuracy: 0.5165\n",
      "Epoch 49/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6882 - binary_accuracy: 0.5168\n",
      "Epoch 50/50\n",
      "1157/1157 [==============================] - 17s 14ms/step - loss: 0.6881 - binary_accuracy: 0.5171\n"
     ]
    }
   ],
   "source": [
    "history = train_model(neural_network, train_samples, train_labels_1st_bit, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Here, we evaluate the neural network with the test data.\n",
    "\n",
    "This block stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496/496 [==============================] - 3s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "results = test_model_binary(neural_network, test_samples, test_labels_1st_bit, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 1241671\n",
      "Accuracy: 0.5010081324432232\n"
     ]
    }
   ],
   "source": [
    "print(\"Correct predictions: \" + str(results[\"correct_predictions\"]))\n",
    "print(\"Accuracy: \" + str(results[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' first bit distribution: 0.5017057754267464\n",
      "'0' first bit distribution: 0.49829422457325356\n"
     ]
    }
   ],
   "source": [
    "total_1_bits = sum(test_labels_1st_bit)\n",
    "distr = total_1_bits / len(test_labels_1st_bit)\n",
    "print(\"'1' first bit distribution: \" + str(distr))\n",
    "print(\"'0' first bit distribution: \"+ str(1-distr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(neural_network, \"speck32_64_1st_bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
