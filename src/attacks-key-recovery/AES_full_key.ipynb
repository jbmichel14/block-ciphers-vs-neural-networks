{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AES Key Recovery: full key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this attack, we aim to recover the first bit of the key using a binary classifier. We train a neural network on plaintext-ciphertext pairs as samples and the first bit of the key as labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from dataset.datasets import AESDatasetCiphertextPlaintextPairKey\n",
    "from pipeline import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: dataset/assets/moby-dick.txt\n",
      "Error: dataset/assets/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "data = AESDatasetCiphertextPlaintextPairKey(128, 'large')\n",
    "\n",
    "train_labels, train_samples, test_labels, test_samples = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training Labels Shape: (1445705, 128)\n",
      "===== Label Shape: (128,)\n",
      "===== Training Samples Shape: (1445705, 256)\n",
      "===== Sample Shape: (256,)\n",
      "===== Testing Labels Shape: (619588, 128)\n",
      "===== Testing Samples Shape: (619588, 256)\n"
     ]
    }
   ],
   "source": [
    "get_dataset_info(train_labels, train_samples, test_labels, test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We add 1 preprocessing step:\n",
    "As our goal is only to recover the first bit of the key, we only keep the first bit of the labels to train our network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, LayerNormalization\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters\n",
    "In this code block, we specify most parameters and hyperparameters that will be used in the training of the neural network.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = np.shape(train_samples[0])\n",
    "\n",
    "# output dimension\n",
    "dim = len(train_labels[0])\n",
    "\n",
    "# units per hidden layer\n",
    "units = 1024\n",
    "\n",
    "# 0.1 to 0.001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.01)\n",
    "\n",
    "loss_scc = 'sparse_categorical_crossentropy'\n",
    "loss_mse = 'mse'\n",
    "loss_bce = 'binary_crossentropy'\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "metrics = ['binary_accuracy']\n",
    "epochs = 50\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In this code block, we create the model, according to the parameters and the topology we want to achieve. \n",
    "We then compile it specifying the optimizer, the loss and the metrics we want outputted.\n",
    "\n",
    "Add customization here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 1024)              263168    \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,443,968\n",
      "Trainable params: 1,443,968\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Â Type of model\n",
    "neural_network = Sequential()\n",
    "\n",
    "# Input layer\n",
    "neural_network.add(Input(shape=input_shape))\n",
    "\n",
    "# Hidden layers\n",
    "#neural_network.add(BatchNormalization())\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "neural_network.add(Dense(units=units, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "neural_network.add(Dense(units=dim, activation='sigmoid'))\n",
    "\n",
    "# Summary\n",
    "neural_network.summary()\n",
    "\n",
    "# Compile model\n",
    "neural_network.compile(optimizer=optimizer, loss=loss_mse, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "In this code block, we train the model. It outputs, for each epoch, the loss and metrics.\n",
    "\n",
    "This block mostly stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1446/1446 [==============================] - 54s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.4999\n",
      "Epoch 4/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "1446/1446 [==============================] - 55s 38ms/step - loss: 0.2500 - binary_accuracy: 0.5001\n",
      "Epoch 10/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "1446/1446 [==============================] - 53s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "1446/1446 [==============================] - 50s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "1446/1446 [==============================] - 51s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "1446/1446 [==============================] - 50s 34ms/step - loss: 0.2500 - binary_accuracy: 0.5001\n",
      "Epoch 20/50\n",
      "1446/1446 [==============================] - 49s 34ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "1446/1446 [==============================] - 51s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "1446/1446 [==============================] - 55s 38ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "1446/1446 [==============================] - 58s 40ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "1446/1446 [==============================] - 59s 41ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "1446/1446 [==============================] - 60s 42ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "1446/1446 [==============================] - 59s 41ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "1446/1446 [==============================] - 60s 41ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "1446/1446 [==============================] - 58s 40ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "1446/1446 [==============================] - 50s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "1446/1446 [==============================] - 59s 41ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "1446/1446 [==============================] - 56s 39ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "1446/1446 [==============================] - 54s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "1446/1446 [==============================] - 56s 39ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "1446/1446 [==============================] - 51s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "1446/1446 [==============================] - 50s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "1446/1446 [==============================] - 51s 35ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "1446/1446 [==============================] - 53s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.4999\n",
      "Epoch 39/50\n",
      "1446/1446 [==============================] - 52s 36ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "1446/1446 [==============================] - 53s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "1446/1446 [==============================] - 49s 34ms/step - loss: 0.2500 - binary_accuracy: 0.4999\n",
      "Epoch 42/50\n",
      "1446/1446 [==============================] - 53s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "1446/1446 [==============================] - 54s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "1446/1446 [==============================] - 57s 39ms/step - loss: 0.2500 - binary_accuracy: 0.5001\n",
      "Epoch 45/50\n",
      "1446/1446 [==============================] - 54s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "1446/1446 [==============================] - 54s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "1446/1446 [==============================] - 56s 38ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "1446/1446 [==============================] - 53s 37ms/step - loss: 0.2500 - binary_accuracy: 0.5001\n",
      "Epoch 49/50\n",
      "1446/1446 [==============================] - 55s 38ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "1446/1446 [==============================] - 55s 38ms/step - loss: 0.2500 - binary_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = train_model(neural_network, train_samples, train_labels, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "In this code block, we train the model. It outputs, for each epoch, the loss and metrics.\n",
    "\n",
    "This block mostly stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 8s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "results = test_model(neural_network, test_samples, test_labels, batch_size, ascii_correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "Correct predictions: 0\n",
      "Accuracy: 0.0\n",
      "=====================================\n",
      "Correct bytes: 0\n",
      "Byte Accuracy: 0.0\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=====================================\")\n",
    "print(\"Correct predictions: \" + str(results[\"correct_predictions\"]))\n",
    "print(\"Accuracy: \" + str(results[\"accuracy\"]))\n",
    "print(\"=====================================\")\n",
    "print(\"Correct bytes: \" + str(results[\"correct_bytes\"]))\n",
    "print(\"Byte Accuracy: \" + str(results[\"byte_accuracy\"]))\n",
    "print(\"=====================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
